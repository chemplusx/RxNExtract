{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RxNExtract","text":"<p>A professional-grade system for extracting chemical reaction information from procedure texts using fine-tuned LLM with Dynamic prompting and self grounding.</p> <p> </p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>Advanced AI: Fine-tuned LLM with dynamic prompting and self-grounding</li> <li>Modular Architecture: Clean, maintainable codebase with separation of concerns</li> <li>Multiple Interfaces: CLI, interactive mode, batch processing, and programmatic API</li> <li>Memory Efficient: 4-bit quantization support for deployment on various hardware</li> <li>Comprehensive Analysis: Error analysis, ablation studies, statistical testing, and uncertainty quantification</li> <li>Easy Installation: One-command installation via PyPI, or Docker</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#30-second-demo","title":"30-Second Demo","text":"<pre><code># Install\npip install rxnextract\n\n# Use\npython -c \"\nfrom chemistry_llm import ChemistryReactionExtractor\nextractor = ChemistryReactionExtractor.from_pretrained('chemplusx/rxnextract-complete')\nprocedure = 'Add 5g NaCl to 100mL water and stir for 30 minutes at room temperature.'\nresults = extractor.analyze_procedure(procedure)\nprint('Reactants:', results['extracted_data']['reactants'])\nprint('Conditions:', results['extracted_data']['conditions'])\n\"\n</code></pre>"},{"location":"#try-without-installation","title":"Try Without Installation","text":""},{"location":"#installation-options","title":"\ud83d\udce6 Installation Options","text":""},{"location":"#option-1-pypi-recommended","title":"Option 1: PyPI (Recommended)","text":"<pre><code>pip install rxnextract                # Basic installation\npip install rxnextract[gpu]           # GPU support\npip install rxnextract[full]          # All features\n</code></pre>"},{"location":"#option-2-docker","title":"Option 2: Docker","text":"<pre><code>docker pull chemplusx/rxnextract:latest\ndocker run -it --gpus all chemplusx/rxnextract:latest\n</code></pre>"},{"location":"#option-3-from-source","title":"Option 3: From Source","text":"<pre><code>git clone https://github.com/chemplusx/RxNExtract.git\ncd RxNExtract\npip install -e .\n</code></pre>"},{"location":"#performance-highlights","title":"\ud83c\udfaf Performance Highlights","text":"<p>Our complete framework achieves significant improvements over baseline methods:</p> Metric Baseline RxNExtract Improvement Complete Reaction Accuracy 23.4% 52.1% +122.6% Entity F1 Score 0.674 0.856 +27.0% Role Classification Accuracy 68.2% 85.9% +25.9% Condition F1 Score 0.421 0.689 +63.7% <p>Error Reduction: 47.8-55.2% across all major error categories Statistical Significance: McNemar's \u03c7\u00b2 = 134.67 (p &lt; 0.001), Cohen's d = 0.82</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"Document Description Installation &amp; Setup Guide Detailed installation instructions, system requirements, and configuration Usage Guide &amp; Examples Comprehensive usage examples, API reference, and advanced features Analysis &amp; Evaluation Complete analysis framework, metrics, and research reproducibility Changelog Version history and release notes"},{"location":"#research-applications","title":"\ud83d\udd2c Research Applications","text":"<p>Perfect for: - Chemical Literature Mining: Extract structured reaction data from papers - Procedure Standardization: Convert natural language to structured formats - Database Curation: Automated reaction database construction - Educational Tools: Teaching reaction analysis and extraction - Research Reproducibility: Systematic evaluation of extraction methods</p>"},{"location":"#community-support","title":"\ud83e\udd1d Community &amp; Support","text":""},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcda Documentation: docs.rxnextract.org</li> <li>\ud83d\udc1b Bug Reports: GitHub Issues</li> <li>\ud83d\udcac Discussions: GitHub Discussions</li> <li>\ud83d\udce7 Email: support@rxnextract.org</li> </ul>"},{"location":"#for-experimental-chemists","title":"For Experimental Chemists","text":"<ul> <li>\ud83c\udfaf One-click installations via PyPI and docker</li> <li>\ud83d\udc33 Docker containers for consistent environments</li> <li>\ud83d\udcd6 User-friendly tutorials and examples</li> <li>\ud83c\udf93 Video tutorials and webinars</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<ul> <li>\ud83d\udd27 Extensive API documentation</li> <li>\ud83e\uddea Comprehensive test suite</li> <li>\ud83c\udfd7\ufe0f Modular architecture for easy extension</li> <li>\ud83d\udccb Contributing guidelines and code standards</li> </ul>"},{"location":"#quick-examples","title":"\ud83d\udd11 Quick Examples","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# Initialize extractor\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n\n# Analyze procedure\nprocedure = \"\"\"\nDissolve 5.0 g of benzoic acid in 100 mL of hot water.\nAdd 10 mL of concentrated HCl and cool the solution.\nFilter the precipitated product and wash with cold water.\n\"\"\"\n\nresults = extractor.analyze_procedure(procedure)\nprint(results['extracted_data'])\n</code></pre>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<pre><code># Interactive mode\nrxnextract --interactive\n\n# Batch processing\nrxnextract --input procedures.txt --output results.json\n\n# Single procedure\nrxnextract --procedure \"Add 2g NaCl to 50mL water\"\n</code></pre>"},{"location":"#analysis-research","title":"Analysis &amp; Research","text":"<pre><code>from chemistry_llm.analysis import ErrorAnalyzer, AblationStudy\n\n# Error analysis\nanalyzer = ErrorAnalyzer()\nerror_results = analyzer.analyze_prediction_errors(predictions, ground_truth)\n\n# Ablation study\nablation = AblationStudy(model_path=\"./model\")\nstudy_results = ablation.run_complete_study(test_data, ground_truth)\n</code></pre>"},{"location":"#system-requirements","title":"\ud83c\udfd7\ufe0f System Requirements","text":"Component Minimum Recommended Python 3.8+ 3.9+ RAM 8GB 16GB+ GPU Memory 4GB 12GB+ Storage 20GB 50GB+ CPU 4 cores 8+ cores <p>Note: Requirements are for inference only. Fine-tuning requires additional resources.</p>"},{"location":"#data-and-software-availability","title":"\ud83d\udcca Data and Software Availability","text":"<p>Code Repository: All code used in this study is available under the MIT License at https://github.com/chemplusx/RxNExtract. The MIT License permits unrestricted use, modification, and distribution, making it suitable for both academic research and commercial applications.</p> <p>Pre-trained Models:  - HuggingFace Hub: chemplusx/rxnextract-complete - Model cards with training details, performance metrics, and usage guidelines</p> <p>Package Distribution: - PyPI: <code>pip install rxnextract</code> - Docker Hub: <code>docker pull chemplusx/rxnextract:latest</code></p> <p>Datasets: Training and evaluation datasets are available at Zenodo DOI: 10.5281/zenodo.XXXXXX</p> <p>Reproducibility: Complete analysis scripts and configuration files are provided to reproduce all results presented in the paper.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or sharing use cases, your help makes RxNExtract better.</p>"},{"location":"#quick-contributing-guide","title":"Quick Contributing Guide","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> <li>Make your changes and add tests</li> <li>Ensure all tests pass (<code>python -m pytest</code>)</li> <li>Submit a pull request</li> </ol> <p>See our Contributing Guidelines for detailed instructions.</p>"},{"location":"#license-citation","title":"\ud83d\udcc4 License &amp; Citation","text":"<p>License: This project is licensed under the MIT License - see the LICENSE file for complete terms.</p> <p>Citation: If you use RxNExtract in your research, please cite our paper:</p> <pre><code>@article{rxnextract2025,\n  title={RxNExtract: A Professional-Grade System for Chemical Reaction Extraction using Fine-tuned LLMs},\n  author={[Your Authors]},\n  journal={[Journal Name]},\n  year={2025},\n  doi={[DOI]}\n}\n</code></pre>"},{"location":"#links","title":"\ud83d\udd17 Links","text":"<ul> <li>Homepage: https://github.com/chemplusx/RxNExtract</li> <li>Documentation: https://docs.rxnextract.org</li> <li>PyPI Package: https://pypi.org/project/rxnextract/</li> <li>Docker Images: https://hub.docker.com/r/chemplusx/rxnextract</li> <li>HuggingFace Models: https://huggingface.co/chemplusx/rxnextract-complete</li> <li>Paper: [Link to published paper]</li> </ul>"},{"location":"ANALYSIS/","title":"Analysis &amp; Evaluation Guide","text":"<p>Comprehensive guide to RxNExtract's analysis and evaluation framework for research-grade assessment and reproducibility.</p>"},{"location":"ANALYSIS/#overview","title":"\ud83d\udd0d Overview","text":"<p>RxNExtract includes a comprehensive analysis suite designed for systematic evaluation of chemical reaction extraction performance. This framework enables researchers to:</p> <ul> <li>Analyze extraction errors across different categories and complexity levels</li> <li>Conduct ablation studies to understand component contributions</li> <li>Perform statistical analysis with significance testing</li> <li>Quantify uncertainty and calibrate confidence scores</li> <li>Calculate comprehensive metrics for research reproducibility</li> </ul>"},{"location":"ANALYSIS/#error-analysis-framework","title":"\ud83d\udcca Error Analysis Framework","text":""},{"location":"ANALYSIS/#basic-error-analysis","title":"Basic Error Analysis","text":"<pre><code>from chemistry_llm.analysis import ErrorAnalyzer\n\n# Initialize error analyzer\nerror_analyzer = ErrorAnalyzer()\n\n# Load your predictions and ground truth\npredictions = load_predictions(\"model_predictions.json\")\nground_truth = load_ground_truth(\"ground_truth.json\")\n\n# Comprehensive error analysis\nerror_results = error_analyzer.analyze_prediction_errors(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    method_name=\"RxNExtract-Complete\"\n)\n\n# Generate detailed error report\nreport = error_analyzer.generate_error_report(\n    error_results, \n    output_file=\"error_analysis_report.txt\"\n)\n\nprint(f\"Total errors analyzed: {error_results.total_errors}\")\nprint(f\"Entity recognition errors: {error_results.entity_errors}\")\nprint(f\"Role classification errors: {error_results.role_errors}\")\nprint(f\"Condition extraction errors: {error_results.condition_errors}\")\n</code></pre>"},{"location":"ANALYSIS/#qualitative-analysis-of-reaction-type-extraction","title":"Qualitative Analysis of Reaction Type Extraction","text":""},{"location":"ANALYSIS/#error-categories","title":"Error Categories","text":""},{"location":"ANALYSIS/#1-entity-recognition-errors","title":"1. Entity Recognition Errors","text":"<ul> <li>Missing Entities: Compounds mentioned in text but not extracted</li> <li>False Positives: Non-chemical entities incorrectly identified as compounds</li> <li>Incorrect Entity Types: Chemical compounds misclassified (e.g., salt as organic compound)</li> </ul> <pre><code># Analyze entity recognition errors specifically\nentity_analysis = error_analyzer.analyze_entity_errors(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    include_details=True\n)\n\nprint(\"Entity Recognition Error Breakdown:\")\nprint(f\"Missing entities: {entity_analysis.missing_entities_rate:.1f}%\")\nprint(f\"False positives: {entity_analysis.false_positive_rate:.1f}%\")\nprint(f\"Type misclassification: {entity_analysis.type_error_rate:.1f}%\")\n</code></pre>"},{"location":"ANALYSIS/#2-role-classification-errors","title":"2. Role Classification Errors","text":"<ul> <li>Reactant/Product Confusion: Misassigning chemical roles</li> <li>Catalyst Misidentification: Catalysts incorrectly classified as reactants</li> <li>Solvent Misclassification: Solvents confused with reactants or reagents</li> </ul> <pre><code># Role classification error analysis\nrole_analysis = error_analyzer.analyze_role_classification_errors(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nprint(\"Role Classification Errors:\")\nprint(f\"Reactant/product confusion: {role_analysis.reactant_product_confusion:.1f}%\")\nprint(f\"Catalyst misidentification: {role_analysis.catalyst_errors:.1f}%\")\nprint(f\"Solvent misclassification: {role_analysis.solvent_errors:.1f}%\")\n</code></pre>"},{"location":"ANALYSIS/#3-condition-extraction-errors","title":"3. Condition Extraction Errors","text":"<ul> <li>Missing Temperature/Time: Reaction conditions not extracted</li> <li>Incomplete Procedures: Partial extraction of multi-step processes</li> <li>Unit Conversion Errors: Incorrect handling of measurement units</li> <li>Implicit Condition Interpretation: Failure to infer standard conditions</li> </ul> <pre><code># Condition extraction error analysis\ncondition_analysis = error_analyzer.analyze_condition_extraction_errors(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    include_implicit=True\n)\n\nprint(\"Condition Extraction Errors:\")\nprint(f\"Missing temperature: {condition_analysis.missing_temperature:.1f}%\")\nprint(f\"Missing time: {condition_analysis.missing_time:.1f}%\")\nprint(f\"Incomplete procedures: {condition_analysis.incomplete_procedures:.1f}%\")\nprint(f\"Unit errors: {condition_analysis.unit_errors:.1f}%\")\n</code></pre>"},{"location":"ANALYSIS/#4-chain-of-thought-cot-reasoning-failures","title":"4. Chain-of-Thought (CoT) Reasoning Failures","text":"<ul> <li>Implicit Condition Interpretation: Failure to reason about unstated conditions</li> <li>Generic Entity Handling: Inability to resolve generic terms like \"the compound\"</li> <li>Multi-step Confusion: Errors in tracking entities across procedure steps</li> </ul> <pre><code># CoT failure analysis (requires raw model outputs)\ncot_failures = error_analyzer.analyze_cot_failures(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    raw_outputs=raw_model_outputs,\n    reasoning_steps=True\n)\n\nprint(\"Chain-of-Thought Failure Analysis:\")\nprint(f\"Implicit reasoning failures: {cot_failures.implicit_failures:.1f}%\")\nprint(f\"Generic entity failures: {cot_failures.generic_entity_failures:.1f}%\")\nprint(f\"Multi-step tracking errors: {cot_failures.multistep_errors:.1f}%\")\n</code></pre>"},{"location":"ANALYSIS/#method-comparison","title":"Method Comparison","text":"<pre><code># Compare multiple methods\nmethod_results = {\n    'baseline': baseline_error_results,\n    'cot_only': cot_error_results,\n    'complete_framework': complete_error_results\n}\n\n# Perform comparative error analysis\nerror_comparisons = error_analyzer.compare_methods(method_results)\n\n# Generate comparison report\ncomparison_report = error_analyzer.generate_comparison_report(\n    error_comparisons,\n    output_file=\"method_comparison_report.txt\"\n)\n\nprint(\"Error Reduction Summary:\")\nfor comparison in error_comparisons:\n    print(f\"{comparison.error_type}: {comparison.error_reduction:.1f}% reduction\")\n</code></pre>"},{"location":"ANALYSIS/#ablation-studies","title":"\ud83e\uddea Ablation Studies","text":"<p>Systematic component-level performance analysis to understand the contribution of different framework components.</p>"},{"location":"ANALYSIS/#complete-ablation-study","title":"Complete Ablation Study","text":"<pre><code>from chemistry_llm.analysis import AblationStudy\n\n# Initialize ablation study\nablation = AblationStudy(\n    model_path=\"./model\",\n    config_path=\"./config/ablation_config.yaml\"\n)\n\n# Run complete ablation study\nstudy_results = ablation.run_complete_study(\n    test_data=test_procedures,\n    ground_truth=ground_truth,\n    sample_size=1000,\n    stratified=True,  # Stratify by reaction complexity\n    complexity_levels=['simple', 'moderate', 'complex'],\n    random_state=42\n)\n\n# Generate comprehensive ablation report\nreport = ablation.generate_ablation_report(\n    study_results, \n    output_file=\"ablation_study_report.txt\"\n)\n\nprint(\"Ablation Study Results:\")\nfor config_name, metrics in study_results.items():\n    print(f\"{config_name:20}: CRA = {metrics.cra:.3f}, F1 = {metrics.entity_f1:.3f}\")\n</code></pre>"},{"location":"ANALYSIS/#ablation-configurations","title":"Ablation Configurations","text":""},{"location":"ANALYSIS/#1-direct-extraction-baseline","title":"1. Direct Extraction (Baseline)","text":"<p>Basic extraction without any enhancements.</p>"},{"location":"ANALYSIS/#2-structured-output","title":"2. Structured Output","text":"<p>XML-formatted output for better parsing.</p>"},{"location":"ANALYSIS/#3-meta-prompt","title":"3. Meta Prompt","text":"<p>Enhanced prompt engineering with task-specific instructions.</p>"},{"location":"ANALYSIS/#4-chain-of-thought-cot","title":"4. Chain-of-Thought (CoT)","text":"<p>Step-by-step reasoning approach.</p>"},{"location":"ANALYSIS/#5-cot-reflection","title":"5. CoT + Reflection","text":"<p>Chain-of-thought with self-reflection and correction.</p>"},{"location":"ANALYSIS/#6-self-grounding","title":"6. Self-Grounding","text":"<p>Entity validation and consistency checking.</p>"},{"location":"ANALYSIS/#7-complete-framework","title":"7. Complete Framework","text":"<p>All components combined (recommended configuration).</p>"},{"location":"ANALYSIS/#8-iterative-refinement","title":"8. Iterative Refinement","text":"<p>Multi-pass extraction with refinement.</p>"},{"location":"ANALYSIS/#dynamic-prompt-analysis","title":"Dynamic Prompt Analysis","text":"<pre><code># Analyze dynamic prompt component contributions\ndynamic_analysis = ablation.analyze_dynamic_prompt_components(\n    test_sample=test_procedures[:100],\n    truth_sample=ground_truth[:100],\n    prompt_variants=[\n        'basic_prompt',\n        'cot_prompt', \n        'reflective_prompt',\n        'self_grounding_prompt'\n    ]\n)\n\n# Component contribution analysis\ncontributions = ablation.analyze_component_contributions(study_results)\n\nprint(\"Component Contributions:\")\nfor component, contribution in contributions.items():\n    print(f\"{component}: +{contribution:.1f}% CRA improvement\")\n\n# Interaction effects analysis\ninteractions = ablation.analyze_interaction_effects(\n    study_results,\n    components=['cot', 'reflection', 'self_grounding']\n)\n</code></pre>"},{"location":"ANALYSIS/#complexity-stratified-analysis","title":"Complexity-Stratified Analysis","text":"<pre><code># Analyze performance by reaction complexity\ncomplexity_results = ablation.analyze_by_complexity(\n    study_results,\n    complexity_labels=complexity_labels,\n    stratification_method='balanced'  # 'balanced', 'natural', 'quantile'\n)\n\nprint(\"Performance by Complexity:\")\nfor complexity in ['simple', 'moderate', 'complex']:\n    results = complexity_results[complexity]\n    print(f\"{complexity:10}: CRA = {results['cra']:.3f}, \"\n          f\"Entity F1 = {results['entity_f1']:.3f}\")\n\n# Export detailed results\nablation.export_results_to_csv(study_results, \"ablation_detailed_results.csv\")\nablation.export_complexity_analysis(complexity_results, \"complexity_analysis.csv\")\n</code></pre>"},{"location":"ANALYSIS/#statistical-analysis","title":"\ud83d\udcc8 Statistical Analysis","text":"<p>Comprehensive statistical testing for research reproducibility and significance assessment.</p>"},{"location":"ANALYSIS/#pairwise-method-comparison","title":"Pairwise Method Comparison","text":"<pre><code>from chemistry_llm.analysis import StatisticalAnalyzer\n\n# Initialize statistical analyzer\nstats_analyzer = StatisticalAnalyzer(\n    significance_level=0.05,\n    confidence_level=0.95\n)\n\n# Load results from different methods\nbaseline_cra_scores = [r['cra'] for r in baseline_results]\ncomplete_cra_scores = [r['cra'] for r in complete_framework_results]\n\n# Perform pairwise statistical comparison\ncomparison = stats_analyzer.perform_pairwise_comparison(\n    method1_results=baseline_cra_scores,\n    method2_results=complete_cra_scores,\n    method1_name=\"Baseline\",\n    method2_name=\"Complete Framework\",\n    test_type=\"paired_t\",  # 'paired_t', 'wilcoxon', 'mann_whitney'\n    effect_size=True\n)\n\nprint(\"Statistical Comparison Results:\")\nprint(f\"Test statistic: {comparison['statistic']:.4f}\")\nprint(f\"p-value: {comparison['p_value']:.6f}\")\nprint(f\"Effect size (Cohen's d): {comparison['effect_size']:.3f}\")\nprint(f\"95% Confidence Interval: [{comparison['ci_lower']:.3f}, {comparison['ci_upper']:.3f}]\")\nprint(f\"Statistically significant: {comparison['significant']}\")\n</code></pre>"},{"location":"ANALYSIS/#mcnemars-test-for-classification-performance","title":"McNemar's Test for Classification Performance","text":"<pre><code># McNemar's test for paired classification results\nbaseline_correct = [is_completely_correct(pred, truth) \n                   for pred, truth in zip(baseline_results, ground_truth)]\ncomplete_correct = [is_completely_correct(pred, truth) \n                   for pred, truth in zip(complete_results, ground_truth)]\n\nmcnemar_result = stats_analyzer.perform_mcnemar_test(\n    method1_correct=baseline_correct,\n    method2_correct=complete_correct,\n    method1_name=\"Baseline\",\n    method2_name=\"Complete Framework\"\n)\n\nprint(\"McNemar's Test Results:\")\nprint(f\"McNemar's \u03c7\u00b2: {mcnemar_result['statistic']:.2f}\")\nprint(f\"p-value: {mcnemar_result['p_value']:.6f}\")\nprint(f\"Significant improvement: {mcnemar_result['significant']}\")\n\n# Contingency table analysis\nprint(f\"Both correct: {mcnemar_result['both_correct']}\")\nprint(f\"Only method1 correct: {mcnemar_result['only_method1_correct']}\")\nprint(f\"Only method2 correct: {mcnemar_result['only_method2_correct']}\")\nprint(f\"Both incorrect: {mcnemar_result['both_incorrect']}\")\n</code></pre>"},{"location":"ANALYSIS/#anova-for-multiple-method-comparison","title":"ANOVA for Multiple Method Comparison","text":"<pre><code># ANOVA for comparing multiple methods\nmethods_data = {\n    'Baseline': [r['cra'] for r in baseline_results],\n    'CoT-Only': [r['cra'] for r in cot_results],\n    'Self-Grounding': [r['cra'] for r in grounding_results],\n    'Complete Framework': [r['cra'] for r in complete_results]\n}\n\nanova_results = stats_analyzer.perform_anova(\n    groups=methods_data,\n    post_hoc=True,  # Include Tukey's HSD post-hoc tests\n    effect_size=True\n)\n\nprint(\"ANOVA Results:\")\nprint(f\"F-statistic: {anova_results['f_statistic']:.4f}\")\nprint(f\"p-value: {anova_results['p_value']:.6f}\")\nprint(f\"Effect size (\u03b7\u00b2): {anova_results['eta_squared']:.3f}\")\n\n# Post-hoc pairwise comparisons\nif anova_results['significant']:\n    print(\"\\nPost-hoc Pairwise Comparisons:\")\n    for comparison in anova_results['post_hoc']:\n        print(f\"{comparison['group1']} vs {comparison['group2']}: \"\n              f\"p = {comparison['p_value']:.4f}\")\n</code></pre>"},{"location":"ANALYSIS/#bootstrap-confidence-intervals","title":"Bootstrap Confidence Intervals","text":"<pre><code># Bootstrap confidence intervals for robust estimation\nbootstrap_results = stats_analyzer.calculate_bootstrap_confidence_intervals(\n    data=complete_cra_scores,\n    statistic='mean',  # 'mean', 'median', 'std'\n    n_bootstrap=1000,\n    confidence_level=0.95\n)\n\nprint(\"Bootstrap Confidence Intervals:\")\nprint(f\"Mean: {bootstrap_results['mean']:.3f}\")\nprint(f\"95% CI: [{bootstrap_results['ci_lower']:.3f}, {bootstrap_results['ci_upper']:.3f}]\")\nprint(f\"Bootstrap SE: {bootstrap_results['bootstrap_se']:.4f}\")\n</code></pre>"},{"location":"ANALYSIS/#baseline-reproducibility-analysis","title":"Baseline Reproducibility Analysis","text":"<pre><code># Analyze reproducibility of literature baselines\nliterature_results = {\n    'ChemRxnBERT': 0.789,\n    'GPT-3.5': 0.641,\n    'RxnScribe': 0.701\n}\n\nreproduced_results = {\n    'ChemRxnBERT': [0.782, 0.785, 0.779, 0.791, 0.788],\n    'GPT-3.5': [0.634, 0.637, 0.631, 0.645, 0.629],\n    'RxnScribe': [0.695, 0.708, 0.697, 0.705, 0.699]\n}\n\nreproducibility = stats_analyzer.calculate_baseline_reproducibility(\n    literature_results=literature_results,\n    reproduced_results=reproduced_results\n)\n\nprint(\"Baseline Reproducibility Analysis:\")\nfor method, repro_stats in reproducibility.items():\n    print(f\"{method}:\")\n    print(f\"  Literature result: {literature_results[method]:.3f}\")\n    print(f\"  Reproduced mean: {repro_stats['mean']:.3f} \u00b1 {repro_stats['std']:.3f}\")\n    print(f\"  Reproducible: {repro_stats['is_reproducible']}\")\n    print(f\"  95% CI contains literature: {repro_stats['ci_contains_literature']}\")\n</code></pre>"},{"location":"ANALYSIS/#comprehensive-statistical-report","title":"Comprehensive Statistical Report","text":"<pre><code># Generate comprehensive statistical report\nstatistical_data = {\n    'pairwise_comparisons': {\n        'baseline_vs_complete': comparison,\n        'cot_vs_complete': cot_comparison\n    },\n    'mcnemar_tests': {\n        'classification_performance': mcnemar_result\n    },\n    'anova_results': anova_results,\n    'bootstrap_intervals': bootstrap_results,\n    'reproducibility_analysis': reproducibility\n}\n\nstats_report = stats_analyzer.generate_statistical_report(\n    statistical_data,\n    output_file=\"comprehensive_statistical_analysis.txt\"\n)\n\n# Export statistical results to CSV for further analysis\nstats_analyzer.export_statistical_results(\n    statistical_data,\n    \"statistical_results.csv\"\n)\n</code></pre>"},{"location":"ANALYSIS/#uncertainty-quantification","title":"\ud83c\udfaf Uncertainty Quantification","text":"<p>Confidence calibration and uncertainty analysis for robust performance assessment.</p>"},{"location":"ANALYSIS/#calibration-metrics","title":"Calibration Metrics","text":"<pre><code>from chemistry_llm.analysis import UncertaintyQuantifier\n\n# Initialize uncertainty quantifier\nuncertainty = UncertaintyQuantifier()\n\n# Extract confidence scores and binary accuracy\nconfidences = [pred['confidence'] for pred in predictions]\naccuracies = [1.0 if is_completely_correct(pred, truth) else 0.0 \n              for pred, truth in zip(predictions, ground_truth)]\n\n# Calculate calibration metrics\ncalibration_metrics = uncertainty.calculate_calibration_metrics(\n    confidences=confidences,\n    accuracies=accuracies,\n    n_bins=10\n)\n\nprint(\"Calibration Analysis:\")\nprint(f\"Expected Calibration Error (ECE): {calibration_metrics.ece:.4f}\")\nprint(f\"Brier Score: {calibration_metrics.brier_score:.4f}\")\nprint(f\"Reliability: {calibration_metrics.reliability:.4f}\")\nprint(f\"Resolution: {calibration_metrics.resolution:.4f}\")\nprint(f\"Uncertainty: {calibration_metrics.uncertainty:.4f}\")\n</code></pre>"},{"location":"ANALYSIS/#temperature-scaling","title":"Temperature Scaling","text":"<pre><code># Perform temperature scaling for calibration improvement\ncalibrated_probs, optimal_temperature = uncertainty.perform_temperature_scaling(\n    validation_logits=validation_logits,\n    validation_labels=validation_labels,\n    test_logits=test_logits\n)\n\nprint(f\"Optimal temperature: {optimal_temperature:.3f}\")\n\n# Calculate calibration metrics after temperature scaling\ncalibrated_metrics = uncertainty.calculate_calibration_metrics(\n    confidences=calibrated_probs,\n    accuracies=accuracies\n)\n\nprint(\"After Temperature Scaling:\")\nprint(f\"ECE improvement: {calibration_metrics.ece - calibrated_metrics.ece:.4f}\")\nprint(f\"Brier Score improvement: {calibration_metrics.brier_score - calibrated_metrics.brier_score:.4f}\")\n</code></pre>"},{"location":"ANALYSIS/#alternative-calibration-methods","title":"Alternative Calibration Methods","text":"<pre><code># Platt scaling\nplatt_calibrated_probs = uncertainty.perform_platt_scaling(\n    validation_scores=validation_scores,\n    validation_labels=validation_labels,\n    test_scores=test_scores\n)\n\n# Isotonic regression\nisotonic_calibrated_probs = uncertainty.perform_isotonic_regression(\n    validation_scores=validation_scores,\n    validation_labels=validation_labels,\n    test_scores=test_scores\n)\n\n# Compare calibration methods\ncalibration_comparison = uncertainty.compare_calibration_methods(\n    original_probs=confidences,\n    temperature_scaled=calibrated_probs,\n    platt_scaled=platt_calibrated_probs,\n    isotonic_calibrated=isotonic_calibrated_probs,\n    true_labels=accuracies\n)\n\nprint(\"Calibration Method Comparison:\")\nfor method, metrics in calibration_comparison.items():\n    print(f\"{method}: ECE = {metrics['ece']:.4f}, Brier = {metrics['brier_score']:.4f}\")\n</code></pre>"},{"location":"ANALYSIS/#confidence-stratified-analysis","title":"Confidence-Stratified Analysis","text":"<pre><code># Analyze performance by confidence level\nconfidence_analysis = uncertainty.analyze_confidence_stratified_performance(\n    confidences=confidences,\n    accuracies=accuracies,\n    n_strata=5  # Divide into 5 confidence bins\n)\n\nprint(\"Performance by Confidence Level:\")\nfor stratum in confidence_analysis:\n    print(f\"Confidence [{stratum['range'][0]:.1f}, {stratum['range'][1]:.1f}]: \"\n          f\"Accuracy = {stratum['accuracy']:.3f}, \"\n          f\"Count = {stratum['count']}\")\n\n# High-confidence performance analysis\nhigh_confidence_threshold = 0.8\nhigh_conf_predictions = [pred for pred in predictions if pred['confidence'] &gt;= high_confidence_threshold]\nhigh_conf_accuracy = calculate_accuracy(high_conf_predictions, corresponding_ground_truth)\n\nprint(f\"High-confidence (\u2265{high_confidence_threshold}) performance:\")\nprint(f\"Coverage: {len(high_conf_predictions)/len(predictions):.1%}\")\nprint(f\"Accuracy: {high_conf_accuracy:.3f}\")\n</code></pre>"},{"location":"ANALYSIS/#reliability-diagrams","title":"Reliability Diagrams","text":"<pre><code># Generate reliability diagram\nreliability_fig = uncertainty.generate_reliability_diagram(\n    confidences=confidences,\n    accuracies=accuracies,\n    n_bins=10,\n    save_path=\"reliability_diagram.png\",\n    title=\"Model Calibration - Reliability Diagram\"\n)\n\n# Generate calibration comparison plot\ncomparison_fig = uncertainty.plot_calibration_comparison(\n    {\n        'Original': confidences,\n        'Temperature Scaled': calibrated_probs,\n        'Platt Scaling': platt_calibrated_probs\n    },\n    true_labels=accuracies,\n    save_path=\"calibration_comparison.png\"\n)\n</code></pre>"},{"location":"ANALYSIS/#comprehensive-uncertainty-analysis","title":"Comprehensive Uncertainty Analysis","text":"<pre><code># Comprehensive uncertainty analysis\nuncertainty_results = uncertainty.analyze_prediction_uncertainty(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    confidence_threshold=0.8,\n    include_calibration=True,\n    include_stratification=True\n)\n\n# Generate uncertainty report\nuncertainty_report = uncertainty.generate_uncertainty_report(\n    uncertainty_results,\n    output_file=\"uncertainty_analysis_report.txt\"\n)\n\nprint(\"Uncertainty Analysis Summary:\")\nprint(f\"Model calibration (ECE): {uncertainty_results['calibration_metrics']['ece']:.4f}\")\nprint(f\"High-confidence accuracy: {uncertainty_results['high_confidence_accuracy']:.3f}\")\nprint(f\"Uncertainty reduction potential: {uncertainty_results['calibration_improvement']:.1%}\")\n</code></pre>"},{"location":"ANALYSIS/#comprehensive-metrics","title":"\ud83d\udccf Comprehensive Metrics","text":"<p>Calculate all performance metrics used in the research paper.</p>"},{"location":"ANALYSIS/#core-metrics","title":"Core Metrics","text":"<pre><code>from chemistry_llm.analysis import MetricsCalculator\n\n# Initialize metrics calculator\nmetrics_calc = MetricsCalculator()\n\n# Calculate comprehensive metrics\ncomprehensive_metrics = metrics_calc.calculate_comprehensive_metrics(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    include_detailed_breakdown=True\n)\n\nprint(\"Core Performance Metrics:\")\nprint(f\"Complete Reaction Accuracy (CRA): {comprehensive_metrics['cra']:.3f}\")\nprint(f\"Entity F1 Score: {comprehensive_metrics['entity_f1']:.3f}\")\nprint(f\"Role Classification Accuracy (RCA): {comprehensive_metrics['rca']:.3f}\")\nprint(f\"Condition Extraction F1: {comprehensive_metrics['condition_f1']:.3f}\")\n\n# Detailed breakdown\nprint(\"\\nDetailed Metrics Breakdown:\")\nprint(f\"Reactant F1: {comprehensive_metrics['reactant_f1']:.3f}\")\nprint(f\"Product F1: {comprehensive_metrics['product_f1']:.3f}\")\nprint(f\"Reagent F1: {comprehensive_metrics['reagent_f1']:.3f}\")\nprint(f\"Solvent F1: {comprehensive_metrics['solvent_f1']:.3f}\")\nprint(f\"Temperature Extraction Accuracy: {comprehensive_metrics['temperature_accuracy']:.3f}\")\nprint(f\"Time Extraction Accuracy: {comprehensive_metrics['time_accuracy']:.3f}\")\n</code></pre>"},{"location":"ANALYSIS/#performance-by-complexity","title":"Performance by Complexity","text":"<pre><code># Assign complexity labels to reactions\ncomplexity_labels = assign_complexity_labels(ground_truth)  # Your implementation\n\n# Calculate performance by reaction complexity\ncomplexity_metrics = metrics_calc.analyze_performance_by_complexity(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    complexity_labels=complexity_labels\n)\n\nprint(\"Performance by Reaction Complexity:\")\nfor complexity, metrics in complexity_metrics.items():\n    print(f\"{complexity.capitalize()}:\")\n    print(f\"  Count: {metrics['count']}\")\n    print(f\"  CRA: {metrics['cra']:.3f}\")\n    print(f\"  Entity F1: {metrics['entity_f1']:.3f}\")\n    print(f\"  RCA: {metrics['rca']:.3f}\")\n</code></pre>"},{"location":"ANALYSIS/#error-reduction-analysis","title":"Error Reduction Analysis","text":"<pre><code># Calculate error reduction compared to baseline\nbaseline_metrics = calculate_baseline_metrics(baseline_predictions, ground_truth)\n\nerror_reduction = metrics_calc.calculate_error_reduction(\n    baseline_metrics=baseline_metrics,\n    improved_metrics=comprehensive_metrics,\n    detailed=True\n)\n\nprint(\"Error Reduction Analysis:\")\nprint(f\"Overall Error Reduction: {error_reduction['overall']:.1f}%\")\nprint(f\"Entity Recognition: {error_reduction['entity_recognition']:.1f}%\")\nprint(f\"Role Classification: {error_reduction['role_classification']:.1f}%\")\nprint(f\"Condition Extraction: {error_reduction['condition_extraction']:.1f}%\")\n\n# Statistical significance of error reduction\nerror_significance = metrics_calc.test_error_reduction_significance(\n    baseline_results=baseline_predictions,\n    improved_results=predictions,\n    ground_truth=ground_truth,\n    test_type='mcnemar'\n)\n\nprint(f\"Error reduction significance: p = {error_significance['p_value']:.6f}\")\n</code></pre>"},{"location":"ANALYSIS/#custom-metrics","title":"Custom Metrics","text":"<pre><code># Define custom evaluation metrics\ndef calculate_yield_extraction_accuracy(predictions, ground_truth):\n    \"\"\"Custom metric for yield extraction accuracy\"\"\"\n    correct_yields = 0\n    total_yields = 0\n\n    for pred, truth in zip(predictions, ground_truth):\n        truth_yields = extract_yields_from_truth(truth)  # Your implementation\n        pred_yields = extract_yields_from_prediction(pred)  # Your implementation\n\n        total_yields += len(truth_yields)\n        correct_yields += count_correct_yields(pred_yields, truth_yields)  # Your implementation\n\n    return correct_yields / total_yields if total_yields &gt; 0 else 0.0\n\n# Calculate custom metrics\nyield_accuracy = calculate_yield_extraction_accuracy(predictions, ground_truth)\nstoichiometry_accuracy = calculate_stoichiometry_accuracy(predictions, ground_truth)\n\nprint(\"Custom Metrics:\")\nprint(f\"Yield Extraction Accuracy: {yield_accuracy:.3f}\")\nprint(f\"Stoichiometry Accuracy: {stoichiometry_accuracy:.3f}\")\n\n# Add to comprehensive metrics\ncomprehensive_metrics.update({\n    'yield_accuracy': yield_accuracy,\n    'stoichiometry_accuracy': stoichiometry_accuracy\n})\n</code></pre>"},{"location":"ANALYSIS/#export-and-visualization","title":"Export and Visualization","text":"<pre><code># Export comprehensive metrics\nmetrics_calc.export_metrics_summary(\n    comprehensive_metrics, \n    \"comprehensive_metrics_summary.json\"\n)\n\n# Export detailed breakdown\nmetrics_calc.export_detailed_metrics(\n    comprehensive_metrics,\n    \"detailed_metrics_breakdown.csv\"\n)\n\n# Generate metrics visualization\nmetrics_viz = metrics_calc.generate_metrics_visualization(\n    comprehensive_metrics,\n    save_path=\"metrics_visualization.png\",\n    include_comparison=True\n)\n\n# Performance heatmap by complexity and metric\nheatmap_fig = metrics_calc.create_performance_heatmap(\n    complexity_metrics,\n    metrics=['cra', 'entity_f1', 'rca', 'condition_f1'],\n    save_path=\"performance_heatmap.png\"\n)\n</code></pre>"},{"location":"ANALYSIS/#research-reproducibility","title":"\ud83d\udd2c Research Reproducibility","text":""},{"location":"ANALYSIS/#complete-analysis-pipeline","title":"Complete Analysis Pipeline","text":"<pre><code>from chemistry_llm.analysis import (\n    ErrorAnalyzer, AblationStudy, UncertaintyQuantifier,\n    StatisticalAnalyzer, MetricsCalculator\n)\n\ndef run_paper_reproduction_analysis(\n    model_path: str,\n    test_data: List[str],\n    ground_truth: List[dict],\n    output_dir: str = \"./analysis_results\"\n):\n    \"\"\"\n    Complete analysis pipeline for research paper reproduction\n\n    This function runs all analyses described in the RxNExtract paper:\n    1. Error analysis with categorization\n    2. Ablation study across all configurations\n    3. Statistical significance testing\n    4. Uncertainty quantification and calibration\n    5. Comprehensive metrics calculation\n    \"\"\"\n\n    import os\n    from pathlib import Path\n\n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True, parents=True)\n\n    print(\"=\" * 80)\n    print(\"RXNEXTRACT COMPREHENSIVE ANALYSIS PIPELINE\")\n    print(\"=\" * 80)\n\n    # 1. Generate predictions for all ablation configurations\n    print(\"\\n1. RUNNING ABLATION STUDY...\")\n    ablation = AblationStudy(model_path=model_path)\n\n    study_results = ablation.run_complete_study(\n        test_data=test_data,\n        ground_truth=ground_truth,\n        sample_size=len(test_data),\n        stratified=True,\n        save_predictions=True,\n        output_dir=str(output_path / \"ablation_predictions\")\n    )\n\n    # Save ablation results\n    ablation.generate_ablation_report(\n        study_results, \n        str(output_path / \"ablation_study_report.txt\")\n    )\n    ablation.export_results_to_csv(\n        study_results, \n        str(output_path / \"ablation_results.csv\")\n    )\n\n    print(f\"Ablation study complete. Results saved to {output_path}\")\n\n    # 2. Error analysis for each configuration\n    print(\"\\n2. RUNNING ERROR ANALYSIS...\")\n    error_analyzer = ErrorAnalyzer()\n\n    method_error_results = {}\n    for config_name, config_results in study_results.items():\n        predictions = config_results.predictions\n\n        error_results = error_analyzer.analyze_prediction_errors(\n            predictions=predictions,\n            ground_truth=ground_truth,\n            method_name=config_name\n        )\n        method_error_results[config_name] = error_results\n\n        # Generate individual error reports\n        error_analyzer.generate_error_report(\n            error_results,\n            str(output_path / f\"error_analysis_{config_name}.txt\")\n        )\n\n    # Comparative error analysis\n    error_comparisons = error_analyzer.compare_methods(method_error_results)\n    error_analyzer.generate_comparison_report(\n        error_comparisons,\n        str(output_path / \"error_reduction_analysis.txt\")\n    )\n\n    print(\"Error analysis complete.\")\n\n    # 3. Statistical analysis\n    print(\"\\n3. RUNNING STATISTICAL ANALYSIS...\")\n    stats_analyzer = StatisticalAnalyzer()\n\n    # Extract CRA scores for statistical testing\n    baseline_cra = [r.cra for r in study_results['direct_extraction'].results]\n    complete_cra = [r.cra for r in study_results['complete_framework'].results]\n\n    # Pairwise comparison\n    statistical_comparison = stats_analyzer.perform_pairwise_comparison(\n        method1_results=baseline_cra,\n        method2_results=complete_cra,\n        method1_name=\"Baseline\",\n        method2_name=\"Complete Framework\",\n        test_type=\"paired_t\"\n    )\n\n    # McNemar's test\n    baseline_correct = [r.is_completely_correct for r in study_results['direct_extraction'].results]\n    complete_correct = [r.is_completely_correct for r in study_results['complete_framework'].results]\n\n    mcnemar_result = stats_analyzer.perform_mcnemar_test(\n        method1_correct=baseline_correct,\n        method2_correct=complete_correct,\n        method1_name=\"Baseline\",\n        method2_name=\"Complete Framework\"\n    )\n\n    # ANOVA across all configurations\n    methods_cra_data = {\n        config: [r.cra for r in results.results]\n        for config, results in study_results.items()\n    }\n\n    anova_results = stats_analyzer.perform_anova(\n        groups=methods_cra_data,\n        post_hoc=True\n    )\n\n    # Generate statistical report\n    statistical_data = {\n        'pairwise_comparisons': {'baseline_vs_complete': statistical_comparison},\n        'mcnemar_tests': {'classification_performance': mcnemar_result},\n        'anova_results': anova_results\n    }\n\n    stats_analyzer.generate_statistical_report(\n        statistical_data,\n        str(output_path / \"statistical_analysis_report.txt\")\n    )\n\n    print(\"Statistical analysis complete.\")\n\n    # 4. Uncertainty quantification\n    print(\"\\n4. RUNNING UNCERTAINTY QUANTIFICATION...\")\n    uncertainty = UncertaintyQuantifier()\n\n    # Use complete framework predictions for uncertainty analysis\n    complete_predictions = study_results['complete_framework'].predictions\n\n    if hasattr(complete_predictions[0], 'confidence'):\n        confidences = [p.confidence for p in complete_predictions]\n        accuracies = [1.0 if p.is_completely_correct else 0.0 for p in complete_predictions]\n\n        # Calibration analysis\n        calibration_metrics = uncertainty.calculate_calibration_metrics(\n            confidences=confidences,\n            accuracies=accuracies\n        )\n\n        # Temperature scaling (requires validation data)\n        if hasattr(complete_predictions[0], 'logits'):\n            val_logits = [p.logits for p in complete_predictions[:200]]  # Use first 200 for validation\n            val_labels = accuracies[:200]\n            test_logits = [p.logits for p in complete_predictions[200:]]\n\n            calibrated_probs, optimal_temp = uncertainty.perform_temperature_scaling(\n                validation_logits=val_logits,\n                validation_labels=val_labels,\n                test_logits=test_logits\n            )\n\n            # Calibration improvement\n            calibrated_metrics = uncertainty.calculate_calibration_metrics(\n                confidences=calibrated_probs,\n                accuracies=accuracies[200:]\n            )\n\n        # Generate reliability diagram\n        uncertainty.generate_reliability_diagram(\n            confidences=confidences,\n            accuracies=accuracies,\n            save_path=str(output_path / \"reliability_diagram.png\")\n        )\n\n        # Comprehensive uncertainty analysis\n        uncertainty_results = uncertainty.analyze_prediction_uncertainty(\n            predictions=complete_predictions,\n            ground_truth=ground_truth\n        )\n\n        uncertainty.generate_uncertainty_report(\n            uncertainty_results,\n            str(output_path / \"uncertainty_analysis_report.txt\")\n        )\n\n        print(\"Uncertainty quantification complete.\")\n\n    else:\n        print(\"Warning: Predictions do not contain confidence scores. Skipping uncertainty analysis.\")\n\n    # 5. Comprehensive metrics calculation\n    print(\"\\n5. CALCULATING COMPREHENSIVE METRICS...\")\n    metrics_calc = MetricsCalculator()\n\n    # Calculate metrics for all configurations\n    all_metrics = {}\n    for config_name, config_results in study_results.items():\n        config_metrics = metrics_calc.calculate_comprehensive_metrics(\n            predictions=config_results.predictions,\n            ground_truth=ground_truth\n        )\n        all_metrics[config_name] = config_metrics\n\n    # Export comprehensive metrics\n    metrics_calc.export_all_metrics(\n        all_metrics,\n        str(output_path / \"comprehensive_metrics.json\")\n    )\n\n    # Create metrics comparison table (Table 3 from paper)\n    metrics_comparison_df = metrics_calc.create_metrics_table(\n        all_metrics,\n        metrics=['cra', 'entity_f1', 'rca', 'condition_f1']\n    )\n    metrics_comparison_df.to_csv(str(output_path / \"metrics_comparison_table.csv\"))\n\n    print(\"Comprehensive metrics calculation complete.\")\n\n    # 6. Generate paper figures and tables\n    print(\"\\n6. GENERATING PAPER FIGURES...\")\n\n    # Error reduction figure (Figure 4 from paper)\n    create_error_reduction_figure(\n        error_comparisons,\n        save_path=str(output_path / \"error_reduction_figure.png\")\n    )\n\n    # Performance by complexity figure\n    create_complexity_performance_figure(\n        study_results,\n        ground_truth,\n        save_path=str(output_path / \"complexity_performance_figure.png\")\n    )\n\n    # Calibration comparison figure\n    if 'uncertainty_results' in locals():\n        create_calibration_figure(\n            uncertainty_results,\n            save_path=str(output_path / \"calibration_figure.png\")\n        )\n\n    print(\"Figure generation complete.\")\n\n    # 7. Generate summary report\n    print(\"\\n7. GENERATING SUMMARY REPORT...\")\n\n    summary_report = generate_comprehensive_summary(\n        ablation_results=study_results,\n        error_analysis=method_error_results,\n        statistical_analysis=statistical_data,\n        uncertainty_analysis=uncertainty_results if 'uncertainty_results' in locals() else None,\n        metrics_analysis=all_metrics\n    )\n\n    with open(str(output_path / \"comprehensive_analysis_summary.txt\"), 'w') as f:\n        f.write(summary_report)\n\n    print(\"=\" * 80)\n    print(\"ANALYSIS PIPELINE COMPLETE!\")\n    print(f\"All results saved to: {output_path}\")\n    print(\"=\" * 80)\n\n    return {\n        'ablation_results': study_results,\n        'error_analysis': method_error_results,\n        'statistical_analysis': statistical_data,\n        'uncertainty_analysis': uncertainty_results if 'uncertainty_results' in locals() else None,\n        'metrics_analysis': all_metrics,\n        'output_directory': str(output_path)\n    }\n\n\ndef generate_comprehensive_summary(\n    ablation_results, error_analysis, statistical_analysis, \n    uncertainty_analysis, metrics_analysis\n):\n    \"\"\"Generate a comprehensive summary report\"\"\"\n\n    summary = []\n    summary.append(\"=\" * 80)\n    summary.append(\"RXNEXTRACT COMPREHENSIVE ANALYSIS SUMMARY\")\n    summary.append(\"=\" * 80)\n    summary.append(\"\")\n\n    # Performance highlights\n    summary.append(\"PERFORMANCE HIGHLIGHTS\")\n    summary.append(\"-\" * 40)\n    baseline_cra = metrics_analysis['direct_extraction']['cra']\n    complete_cra = metrics_analysis['complete_framework']['cra']\n    improvement = ((complete_cra - baseline_cra) / baseline_cra) * 100\n\n    summary.append(f\"Complete Reaction Accuracy:\")\n    summary.append(f\"  Baseline: {baseline_cra:.1%}\")\n    summary.append(f\"  Complete Framework: {complete_cra:.1%}\")\n    summary.append(f\"  Improvement: +{improvement:.1f}%\")\n    summary.append(\"\")\n\n    # Error reduction summary\n    summary.append(\"ERROR REDUCTION SUMMARY\")\n    summary.append(\"-\" * 40)\n\n    baseline_errors = error_analysis['direct_extraction']\n    complete_errors = error_analysis['complete_framework']\n\n    entity_reduction = ((baseline_errors.entity_errors - complete_errors.entity_errors) / \n                       baseline_errors.entity_errors) * 100\n    role_reduction = ((baseline_errors.role_errors - complete_errors.role_errors) / \n                     baseline_errors.role_errors) * 100\n    condition_reduction = ((baseline_errors.condition_errors - complete_errors.condition_errors) / \n                          baseline_errors.condition_errors) * 100\n\n    summary.append(f\"Entity Recognition Errors: -{entity_reduction:.1f}%\")\n    summary.append(f\"Role Classification Errors: -{role_reduction:.1f}%\")\n    summary.append(f\"Condition Extraction Errors: -{condition_reduction:.1f}%\")\n    summary.append(\"\")\n\n    # Statistical significance\n    summary.append(\"STATISTICAL SIGNIFICANCE\")\n    summary.append(\"-\" * 40)\n    pairwise = statistical_analysis['pairwise_comparisons']['baseline_vs_complete']\n    mcnemar = statistical_analysis['mcnemar_tests']['classification_performance']\n\n    summary.append(f\"Paired t-test p-value: {pairwise['p_value']:.6f}\")\n    summary.append(f\"Effect size (Cohen's d): {pairwise['effect_size']:.3f}\")\n    summary.append(f\"McNemar's \u03c7\u00b2: {mcnemar['statistic']:.2f} (p = {mcnemar['p_value']:.6f})\")\n    summary.append(\"\")\n\n    # Uncertainty quantification\n    if uncertainty_analysis:\n        summary.append(\"UNCERTAINTY QUANTIFICATION\")\n        summary.append(\"-\" * 40)\n        summary.append(f\"Expected Calibration Error: {uncertainty_analysis['calibration_metrics']['ece']:.4f}\")\n        summary.append(f\"Brier Score: {uncertainty_analysis['calibration_metrics']['brier_score']:.4f}\")\n        summary.append(f\"High-confidence accuracy: {uncertainty_analysis['high_confidence_accuracy']:.3f}\")\n        summary.append(\"\")\n\n    # Component contributions\n    summary.append(\"COMPONENT CONTRIBUTIONS\")\n    summary.append(\"-\" * 40)\n\n    configs = ['direct_extraction', 'structured_output', 'meta_prompt', \n               'chain_of_thought', 'cot_reflection', 'self_grounding', 'complete_framework']\n\n    prev_cra = 0\n    for config in configs:\n        if config in metrics_analysis:\n            cra = metrics_analysis[config]['cra']\n            contribution = cra - prev_cra\n            summary.append(f\"{config.replace('_', ' ').title()}: {cra:.3f} (+{contribution:.3f})\")\n            prev_cra = cra\n\n    summary.append(\"\")\n    summary.append(\"=\" * 80)\n\n    return \"\\n\".join(summary)\n\n\n# Helper functions for figure generation\ndef create_error_reduction_figure(error_comparisons, save_path):\n    \"\"\"Create error reduction visualization (Figure 4 from paper)\"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Extract error reduction data\n    categories = ['Entity Recognition', 'Role Classification', 'Condition Extraction']\n    reductions = []\n\n    for comparison in error_comparisons:\n        if 'entity' in comparison.error_type.lower():\n            reductions.append(comparison.error_reduction)\n        elif 'role' in comparison.error_type.lower():\n            reductions.append(comparison.error_reduction)\n        elif 'condition' in comparison.error_type.lower():\n            reductions.append(comparison.error_reduction)\n\n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.bar(categories, reductions, color=['#2E86AB', '#A23B72', '#F18F01'])\n\n    # Add value labels on bars\n    for bar, reduction in zip(bars, reductions):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{reduction:.1f}%',\n                ha='center', va='bottom', fontweight='bold')\n\n    ax.set_ylabel('Error Reduction (%)', fontsize=12)\n    ax.set_title('Error Reduction by Category', fontsize=14, fontweight='bold')\n    ax.set_ylim(0, max(reductions) * 1.1)\n\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef create_complexity_performance_figure(study_results, ground_truth, save_path):\n    \"\"\"Create performance by complexity visualization\"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # This would require implementing complexity assignment\n    # Placeholder for the actual implementation\n    complexities = ['Simple', 'Moderate', 'Complex']\n    configs = ['Baseline', 'CoT', 'Complete Framework']\n\n    # Mock data - replace with actual complexity analysis\n    data = np.array([\n        [0.45, 0.35, 0.25],  # Baseline\n        [0.52, 0.41, 0.32],  # CoT\n        [0.65, 0.51, 0.42]   # Complete\n    ])\n\n    # Create grouped bar plot\n    x = np.arange(len(complexities))\n    width = 0.25\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for i, config in enumerate(configs):\n        ax.bar(x + i*width, data[i], width, label=config)\n\n    ax.set_xlabel('Reaction Complexity')\n    ax.set_ylabel('Complete Reaction Accuracy')\n    ax.set_title('Performance by Reaction Complexity')\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(complexities)\n    ax.legend()\n\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef create_calibration_figure(uncertainty_results, save_path):\n    \"\"\"Create calibration visualization\"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Extract calibration data\n    calibration_data = uncertainty_results['calibration_metrics']\n\n    # Create reliability diagram\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Reliability diagram\n    bin_boundaries = np.linspace(0, 1, 11)\n    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n\n    # Mock reliability data - replace with actual data\n    observed_frequencies = bin_centers + np.random.normal(0, 0.05, len(bin_centers))\n    observed_frequencies = np.clip(observed_frequencies, 0, 1)\n\n    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n    ax1.plot(bin_centers, observed_frequencies, 'ro-', label='Model calibration')\n    ax1.set_xlabel('Mean Predicted Probability')\n    ax1.set_ylabel('Fraction of Positives')\n    ax1.set_title('Reliability Diagram')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Confidence distribution\n    confidences = np.random.beta(2, 2, 1000)  # Mock data\n    ax2.hist(confidences, bins=20, alpha=0.7, edgecolor='black')\n    ax2.set_xlabel('Confidence Score')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Confidence Distribution')\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n# Example usage of the complete analysis pipeline\nif __name__ == \"__main__\":\n    # Load your data\n    test_procedures = load_test_data(\"test_procedures.json\")\n    ground_truth = load_ground_truth(\"ground_truth.json\")\n\n    # Run complete analysis\n    results = run_paper_reproduction_analysis(\n        model_path=\"./path/to/model\",\n        test_data=test_procedures,\n        ground_truth=ground_truth,\n        output_dir=\"./paper_reproduction_results\"\n    )\n\n    print(\"Paper reproduction analysis complete!\")\n    print(f\"Results available in: {results['output_directory']}\")\n</code></pre>"},{"location":"ANALYSIS/#command-line-analysis-scripts","title":"\ud83d\udee0\ufe0f Command Line Analysis Scripts","text":"<p>For convenience, RxNExtract provides command-line scripts for all analysis functions:</p>"},{"location":"ANALYSIS/#error-analysis-script","title":"Error Analysis Script","text":"<pre><code># Run comprehensive error analysis\npython scripts/run_error_analysis.py \\\n    --predictions model_predictions.json \\\n    --ground-truth ground_truth.json \\\n    --method-name \"Complete Framework\" \\\n    --output-dir ./analysis_output \\\n    --include-cot-analysis \\\n    --raw-outputs raw_model_outputs.json\n\n# Compare multiple methods\npython scripts/run_error_analysis.py \\\n    --predictions-files baseline.json cot.json complete.json \\\n    --method-names \"Baseline\" \"CoT\" \"Complete\" \\\n    --ground-truth ground_truth.json \\\n    --output-dir ./comparison_output \\\n    --generate-comparison-report\n</code></pre>"},{"location":"ANALYSIS/#ablation-study-script","title":"Ablation Study Script","text":"<pre><code># Run complete ablation study\npython scripts/run_ablation_study.py \\\n    --model-path ./model \\\n    --test-data test_procedures.json \\\n    --ground-truth ground_truth.json \\\n    --output-dir ./ablation_output \\\n    --sample-size 1000 \\\n    --stratified \\\n    --complexity-analysis \\\n    --dynamic-prompt-analysis\n\n# Custom configuration ablation\npython scripts/run_ablation_study.py \\\n    --config custom_ablation_config.yaml \\\n    --test-data test_procedures.json \\\n    --ground-truth ground_truth.json \\\n    --output-dir ./custom_ablation\n</code></pre>"},{"location":"ANALYSIS/#statistical-analysis-script","title":"Statistical Analysis Script","text":"<pre><code># Pairwise method comparison\npython scripts/run_statistical_analysis.py \\\n    --method1-results baseline_results.json \\\n    --method2-results framework_results.json \\\n    --method-names \"Baseline\" \"Complete Framework\" \\\n    --output-dir ./stats_output \\\n    --tests paired_t mcnemar bootstrap\n\n# Multiple method ANOVA\npython scripts/run_statistical_analysis.py \\\n    --results-files baseline.json cot.json grounding.json complete.json \\\n    --method-names \"Baseline\" \"CoT\" \"Self-Grounding\" \"Complete\" \\\n    --output-dir ./anova_output \\\n    --test anova \\\n    --post-hoc-tests\n</code></pre>"},{"location":"ANALYSIS/#uncertainty-analysis-script","title":"Uncertainty Analysis Script","text":"<pre><code># Calibration analysis\npython scripts/run_uncertainty_analysis.py \\\n    --predictions predictions_with_confidence.json \\\n    --ground-truth ground_truth.json \\\n    --output-dir ./uncertainty_output \\\n    --calibration-methods temperature_scaling platt_scaling isotonic \\\n    --generate-plots \\\n    --confidence-analysis\n\n# With validation data for temperature scaling\npython scripts/run_uncertainty_analysis.py \\\n    --predictions predictions.json \\\n    --ground-truth ground_truth.json \\\n    --validation-data validation_data.json \\\n    --output-dir ./uncertainty_output \\\n    --temperature-scaling \\\n    --reliability-diagram\n</code></pre>"},{"location":"ANALYSIS/#complete-analysis-pipeline-script","title":"Complete Analysis Pipeline Script","text":"<pre><code># Run full paper reproduction analysis\npython scripts/run_complete_analysis.py \\\n    --model-path ./model \\\n    --test-data test_procedures.json \\\n    --ground-truth ground_truth.json \\\n    --output-dir ./complete_analysis \\\n    --config analysis_config.yaml \\\n    --generate-figures \\\n    --export-tables\n\n# Quick analysis with default settings\npython scripts/run_complete_analysis.py \\\n    --model-path ./model \\\n    --test-data test_procedures.json \\\n    --ground-truth ground_truth.json \\\n    --quick-analysis\n</code></pre>"},{"location":"ANALYSIS/#analysis-output-structure","title":"\ud83d\udcc1 Analysis Output Structure","text":"<p>The analysis framework generates organized output files:</p> <pre><code>analysis_output/\n\u251c\u2500\u2500 error_analysis/\n\u2502   \u251c\u2500\u2500 error_analysis_results.json\n\u2502   \u251c\u2500\u2500 error_analysis_report.txt\n\u2502   \u251c\u2500\u2500 cot_failure_analysis.json\n\u2502   \u2514\u2500\u2500 method_comparison.json\n\u251c\u2500\u2500 ablation_study/\n\u2502   \u251c\u2500\u2500 ablation_results.json\n\u2502   \u251c\u2500\u2500 ablation_report.txt\n\u2502   \u251c\u2500\u2500 ablation_results.csv\n\u2502   \u2514\u2500\u2500 component_contributions.json\n\u251c\u2500\u2500 statistical_analysis/\n\u2502   \u251c\u2500\u2500 statistical_results.json\n\u2502   \u251c\u2500\u2500 statistical_report.txt\n\u2502   \u251c\u2500\u2500 pairwise_comparisons.csv\n\u2502   \u2514\u2500\u2500 significance_tests.json\n\u251c\u2500\u2500 uncertainty_analysis/\n\u2502   \u251c\u2500\u2500 calibration_metrics.json\n\u2502   \u251c\u2500\u2500 uncertainty_report.txt\n\u2502   \u251c\u2500\u2500 reliability_diagram.png\n\u2502   \u2514\u2500\u2500 confidence_analysis.json\n\u251c\u2500\u2500 metrics/\n\u2502   \u251c\u2500\u2500 comprehensive_metrics.json\n\u2502   \u251c\u2500\u2500 complexity_analysis.csv\n\u2502   \u2514\u2500\u2500 metrics_summary.json\n\u251c\u2500\u2500 figures/\n\u2502   \u251c\u2500\u2500 error_reduction_figure.png\n\u2502   \u251c\u2500\u2500 performance_heatmap.png\n\u2502   \u251c\u2500\u2500 calibration_comparison.png\n\u2502   \u2514\u2500\u2500 complexity_performance.png\n\u2514\u2500\u2500 tables/\n    \u251c\u2500\u2500 metrics_comparison_table.csv\n    \u251c\u2500\u2500 statistical_significance_table.csv\n    \u2514\u2500\u2500 error_reduction_table.csv\n</code></pre>"},{"location":"ANALYSIS/#custom-analysis-configuration","title":"\ud83d\udd27 Custom Analysis Configuration","text":""},{"location":"ANALYSIS/#configuration-file-structure","title":"Configuration File Structure","text":"<p>Create <code>analysis_config.yaml</code>:</p> <pre><code># Analysis Configuration\nanalysis:\n  # Error Analysis Settings\n  error_analysis:\n    include_cot_failures: true\n    categorize_by_complexity: true\n    detailed_breakdown: true\n    confidence_threshold: 0.8\n\n  # Ablation Study Settings  \n  ablation_study:\n    sample_size: 1000\n    stratified_sampling: true\n    complexity_stratification: ['simple', 'moderate', 'complex']\n    random_state: 42\n    include_dynamic_prompt_analysis: true\n    save_intermediate_results: true\n\n  # Statistical Analysis Settings\n  statistical_analysis:\n    significance_level: 0.05\n    confidence_level: 0.95\n    bootstrap_iterations: 1000\n    effect_size_calculation: true\n    normality_tests: true\n\n  # Uncertainty Quantification Settings\n  uncertainty_quantification:\n    calibration_methods: \n      - temperature_scaling\n      - platt_scaling\n      - isotonic_regression\n    n_calibration_bins: 10\n    confidence_threshold: 0.8\n    generate_reliability_plots: true\n    stratified_analysis: true\n\n  # Metrics Calculation Settings\n  metrics:\n    include_detailed_breakdown: true\n    complexity_analysis: true\n    custom_metrics: \n      - yield_extraction_accuracy\n      - stoichiometry_accuracy\n    export_formats: ['json', 'csv']\n\n# Output Settings\noutput:\n  base_directory: \"./analysis_results\"\n  generate_figures: true\n  export_tables: true\n  create_summary_report: true\n  figure_format: \"png\"\n  figure_dpi: 300\n\n# Logging\nlogging:\n  level: \"INFO\"\n  file: \"analysis.log\"\n  include_timestamps: true\n</code></pre>"},{"location":"ANALYSIS/#custom-metrics-implementation","title":"Custom Metrics Implementation","text":"<pre><code>from chemistry_llm.analysis.metrics import BaseMetric\n\nclass YieldExtractionMetric(BaseMetric):\n    \"\"\"Custom metric for yield extraction accuracy\"\"\"\n\n    def calculate(self, predictions, ground_truth):\n        correct_yields = 0\n        total_yields = 0\n\n        for pred, truth in zip(predictions, ground_truth):\n            # Extract yields from ground truth\n            truth_yields = self.extract_yields(truth)\n            # Extract yields from prediction\n            pred_yields = self.extract_yields(pred['extracted_data'])\n\n            total_yields += len(truth_yields)\n\n            # Count correct yield extractions\n            for truth_yield in truth_yields:\n                if any(self.yields_match(truth_yield, pred_yield) \n                       for pred_yield in pred_yields):\n                    correct_yields += 1\n\n        return {\n            'yield_extraction_accuracy': correct_yields / total_yields if total_yields &gt; 0 else 0.0,\n            'total_yields': total_yields,\n            'correct_yields': correct_yields\n        }\n\n    def extract_yields(self, data):\n        \"\"\"Extract yield information from data structure\"\"\"\n        yields = []\n\n        if isinstance(data, dict):\n            # Extract from products\n            for product in data.get('products', []):\n                if 'yield' in product:\n                    yields.append(product['yield'])\n\n        return yields\n\n    def yields_match(self, truth_yield, pred_yield, tolerance=0.05):\n        \"\"\"Check if predicted yield matches truth yield within tolerance\"\"\"\n        # Implementation for yield matching logic\n        # This would handle percentage parsing, numerical comparison, etc.\n        pass\n\n# Register custom metric\nfrom chemistry_llm.analysis import MetricsCalculator\n\nmetrics_calc = MetricsCalculator()\nmetrics_calc.register_custom_metric('yield_extraction', YieldExtractionMetric())\n</code></pre>"},{"location":"ANALYSIS/#research-applications","title":"\ud83c\udfaf Research Applications","text":""},{"location":"ANALYSIS/#literature-survey-analysis","title":"Literature Survey Analysis","text":"<pre><code>def analyze_extraction_methods_survey():\n    \"\"\"Analyze multiple extraction methods from literature\"\"\"\n\n    # Methods to compare\n    methods = {\n        'ChemRxnBERT': load_chemrxnbert_results(),\n        'GPT-3.5': load_gpt35_results(),\n        'RxnScribe': load_rxnscribe_results(),\n        'RxNExtract': load_rxnextract_results()\n    }\n\n    ground_truth = load_ground_truth()\n\n    # Comprehensive comparison\n    comparison_results = {}\n\n    for method_name, predictions in methods.items():\n        # Error analysis\n        error_results = error_analyzer.analyze_prediction_errors(\n            predictions, ground_truth, method_name\n        )\n\n        # Metrics calculation\n        metrics = metrics_calc.calculate_comprehensive_metrics(\n            predictions, ground_truth\n        )\n\n        # Uncertainty analysis (if confidence available)\n        if hasattr(predictions[0], 'confidence'):\n            uncertainty_results = uncertainty.analyze_prediction_uncertainty(\n                predictions, ground_truth\n            )\n        else:\n            uncertainty_results = None\n\n        comparison_results[method_name] = {\n            'error_analysis': error_results,\n            'metrics': metrics,\n            'uncertainty': uncertainty_results\n        }\n\n    # Statistical comparison\n    method_cra_scores = {\n        name: [calc_cra(pred, truth) for pred, truth in zip(predictions, ground_truth)]\n        for name, predictions in methods.items()\n    }\n\n    anova_results = stats_analyzer.perform_anova(\n        groups=method_cra_scores,\n        post_hoc=True\n    )\n\n    # Generate literature comparison report\n    generate_literature_comparison_report(\n        comparison_results,\n        anova_results,\n        \"literature_survey_analysis.txt\"\n    )\n\n# Run literature survey\nanalyze_extraction_methods_survey()\n</code></pre>"},{"location":"ANALYSIS/#domain-adaptation-analysis","title":"Domain Adaptation Analysis","text":"<pre><code>def analyze_domain_adaptation():\n    \"\"\"Analyze performance across different chemistry domains\"\"\"\n\n    domains = ['organic', 'inorganic', 'polymer', 'catalysis']\n\n    for domain in domains:\n        domain_test_data = load_domain_data(domain)\n        domain_ground_truth = load_domain_ground_truth(domain)\n\n        print(f\"\\nAnalyzing {domain.upper()} chemistry domain:\")\n\n        # Run ablation study for this domain\n        domain_ablation = ablation.run_complete_study(\n            test_data=domain_test_data,\n            ground_truth=domain_ground_truth,\n            sample_size=min(500, len(domain_test_data))\n        )\n\n        # Domain-specific error analysis\n        domain_errors = error_analyzer.analyze_prediction_errors(\n            predictions=domain_ablation['complete_framework'].predictions,\n            ground_truth=domain_ground_truth,\n            method_name=f\"Complete-{domain.title()}\"\n        )\n\n        # Save domain-specific results\n        save_domain_results(domain, domain_ablation, domain_errors)\n\n    # Cross-domain comparison\n    generate_cross_domain_analysis(domains)\n\n# Run domain adaptation analysis\nanalyze_domain_adaptation()\n</code></pre> <p>This comprehensive analysis framework enables researchers to: - Reproduce all results from the RxNExtract research paper - Conduct systematic evaluations of new extraction methods - Perform rigorous statistical analysis for publication-quality research - Quantify uncertainty and calibration for reliable confidence estimation - Generate publication-ready figures and tables automatically</p> <p>For implementation details and advanced customization, refer to the source code documentation in the <code>src/chemistry_llm/analysis/</code> directory.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to the RxNExtract project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#125-2025-09-03","title":"[1.2.5] - 2025-09-03","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>PyPI package distribution for easy installation</li> <li>Docker containerization with GPU support</li> <li>HuggingFace model hub integration</li> <li>Interactive Jupyter notebook examples</li> <li>Web API integration examples</li> <li>Enhanced documentation with comprehensive guides</li> <li>Community support channels and resources</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Restructured documentation into modular guides</li> <li>Improved installation process with multiple options</li> <li>Enhanced CLI interface with better error handling</li> <li>Updated model loading to support HuggingFace Hub</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Memory optimization issues in batch processing</li> <li>Cross-platform compatibility improvements</li> <li>Enhanced error messages and logging</li> </ul>"},{"location":"CHANGELOG/#120-2025-08-21","title":"[1.2.0] - 2025-08-21","text":""},{"location":"CHANGELOG/#added-comprehensive-analysis-suite","title":"Added - Comprehensive Analysis Suite","text":"<ul> <li>Error Analysis Framework: Systematic error categorization and analysis</li> <li>Entity recognition error analysis (missing entities, false positives, incorrect types)</li> <li>Role classification error analysis (reactant/product confusion, catalyst misidentification)</li> <li>Condition extraction error analysis (missing temperature/time, incomplete procedures)</li> <li>Chain-of-Thought failure analysis (implicit reasoning, generic entity handling)</li> <li>Ablation Study Framework: Component-level performance analysis</li> <li>8 ablation configurations from direct extraction to complete framework</li> <li>Dynamic prompt component analysis</li> <li>Complexity-stratified evaluation (simple, moderate, complex reactions)</li> <li>Component contribution and interaction effects analysis</li> <li>Statistical Analysis Suite: Research-grade significance testing</li> <li>Pairwise method comparison (paired t-tests, Wilcoxon signed-rank)</li> <li>McNemar's test for classification performance comparison</li> <li>One-way ANOVA with post-hoc tests for multiple method comparison</li> <li>Bootstrap confidence intervals and effect size calculations</li> <li>Baseline reproducibility analysis for literature validation</li> <li>Uncertainty Quantification Module: Confidence calibration and analysis</li> <li>Expected Calibration Error (ECE) and Brier Score calculation</li> <li>Temperature scaling, Platt scaling, and isotonic regression</li> <li>Confidence-stratified performance analysis</li> <li>Reliability diagram generation and visualization</li> <li>Comprehensive Metrics Calculator: Complete performance assessment</li> <li>Complete Reaction Accuracy (CRA), Entity F1, Role Classification Accuracy</li> <li>Condition extraction metrics and complexity-based analysis</li> <li>Error reduction calculations and custom metrics support</li> <li>Command-line Analysis Scripts: Easy-to-use analysis tools</li> <li><code>run_error_analysis.py</code>: Comprehensive error analysis</li> <li><code>run_ablation_study.py</code>: Complete ablation studies</li> <li><code>run_statistical_analysis.py</code>: Statistical significance testing</li> <li><code>run_uncertainty_analysis.py</code>: Uncertainty quantification</li> <li><code>run_complete_analysis.py</code>: Full analysis pipeline</li> </ul>"},{"location":"CHANGELOG/#added-research-reproducibility-features","title":"Added - Research Reproducibility Features","text":"<ul> <li>Complete Analysis Pipeline: End-to-end analysis workflow</li> <li>Automated generation of all paper figures and tables</li> <li>Research reproducibility with configurable parameters</li> <li>Export functionality for publication-ready results</li> <li>Performance Benchmarking: Systematic evaluation framework</li> <li>Literature baseline comparison and validation</li> <li>Cross-domain performance analysis</li> <li>Complexity-stratified evaluation metrics</li> </ul>"},{"location":"CHANGELOG/#added-enhanced-documentation","title":"Added - Enhanced Documentation","text":"<ul> <li>Analysis Guide: Comprehensive analysis framework documentation</li> <li>Advanced Examples: Research applications and custom analysis</li> <li>Configuration Guide: Detailed configuration options</li> <li>API Reference: Complete analysis module documentation</li> </ul>"},{"location":"CHANGELOG/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Error Reduction: 47.8-55.2% across major error categories</li> <li>Entity Recognition: 52.4% reduction in missing entities, 54.8% in false positives</li> <li>Role Classification: 55.2% reduction in reactant/product confusion</li> <li>Condition Extraction: 49.1% reduction in missing temperature conditions</li> <li>Statistical Significance: McNemar's \u03c7\u00b2 = 134.67 (p &lt; 0.001), Cohen's d = 0.82</li> <li>Calibration Improvement: 57.1% ECE reduction with temperature scaling</li> <li>Overall Performance: +122.6% improvement in Complete Reaction Accuracy</li> </ul>"},{"location":"CHANGELOG/#technical-enhancements","title":"Technical Enhancements","text":"<ul> <li>Enhanced memory efficiency with 4-bit quantization support</li> <li>Improved batch processing with progress tracking</li> <li>Advanced prompt engineering with dynamic selection</li> <li>Robust XML parsing with error recovery</li> <li>Professional logging with configurable levels</li> </ul>"},{"location":"CHANGELOG/#110-2025-06-15","title":"[1.1.0] - 2025-06-15","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Self-Grounding Mechanism: Entity validation and consistency checking</li> <li>Iterative Refinement: Multi-pass extraction with quality improvement</li> <li>Dynamic Prompt Selection: Context-aware prompt optimization</li> <li>Confidence Scoring: Prediction confidence estimation</li> <li>Batch Processing Optimization: Efficient large-scale processing</li> </ul>"},{"location":"CHANGELOG/#improved","title":"Improved","text":"<ul> <li>Chain-of-Thought Reasoning: Enhanced step-by-step extraction</li> <li>XML Structure Validation: Robust output parsing</li> <li>Error Handling: Graceful failure recovery</li> <li>Memory Management: Reduced memory footprint</li> </ul>"},{"location":"CHANGELOG/#performance-gains","title":"Performance Gains","text":"<ul> <li>Complete Reaction Accuracy: Improved from 23.4% to 52.1%</li> <li>Entity F1 Score: Improved from 0.674 to 0.856</li> <li>Role Classification: Improved from 68.2% to 85.9%</li> <li>Condition Extraction: Improved from 0.421 to 0.689 F1</li> </ul>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed memory leaks in long-running processes</li> <li>Resolved parsing errors for complex chemical names</li> <li>Fixed device selection issues on multi-GPU systems</li> </ul>"},{"location":"CHANGELOG/#100-2025-05-21-initial-release","title":"[1.0.0] - 2025-05-21 - Initial Release","text":""},{"location":"CHANGELOG/#added-core-functionality","title":"Added - Core Functionality","text":"<ul> <li>Chemical Reaction Extraction: Core extraction engine using fine-tuned LLMs</li> <li>Multiple Interfaces: CLI, interactive mode, batch processing, programmatic API</li> <li>Chain-of-Thought Prompting: Step-by-step reasoning for better accuracy</li> <li>XML Output Parsing: Structured data output for easy integration</li> <li>CLI Interface: Command-line interface for streamlined usage</li> <li>Comprehensive Test Suite: Unit and integration tests for reliability</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to Chemistry LLM Inference","text":"<p>We welcome contributions to the Chemistry LLM Inference project! This document provides guidelines for contributing.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>Making Changes</li> <li>Testing</li> <li>Submitting Changes</li> <li>Code Style</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a code of conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to the project maintainers.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Set up the development environment</li> <li>Create a new branch for your changes</li> <li>Make your changes</li> <li>Test your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":""},{"location":"CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Git</li> <li>Access to a GPU (recommended) or sufficient CPU resources</li> </ul>"},{"location":"CONTRIBUTING/#environment-setup","title":"Environment Setup","text":"<pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/chemistry-llm-inference.git\ncd chemistry-llm-inference\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements.txt\npip install -e .\n\n# Install additional development tools\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"CONTRIBUTING/#directory-structure","title":"Directory Structure","text":"<pre><code>chemistry-llm-inference/\n\u251c\u2500\u2500 src/chemistry_llm/          # Main package code\n\u251c\u2500\u2500 tests/                      # Test files\n\u251c\u2500\u2500 examples/                   # Example usage scripts\n\u251c\u2500\u2500 scripts/                    # Utility scripts\n\u251c\u2500\u2500 config/                     # Configuration files\n\u2514\u2500\u2500 docs/                       # Documentation (if added)\n</code></pre>"},{"location":"CONTRIBUTING/#making-changes","title":"Making Changes","text":""},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":"<p>We welcome the following types of contributions:</p> <ol> <li>Bug fixes - Fix issues in existing code</li> <li>Feature additions - Add new functionality</li> <li>Documentation improvements - Enhance or clarify documentation</li> <li>Performance optimizations - Improve speed or memory usage</li> <li>Test improvements - Add or improve test coverage</li> </ol>"},{"location":"CONTRIBUTING/#branch-naming","title":"Branch Naming","text":"<p>Use clear, descriptive branch names:</p> <ul> <li><code>feature/add-batch-processing</code></li> <li><code>bugfix/xml-parsing-error</code></li> <li><code>docs/update-readme</code></li> <li><code>refactor/model-loading</code></li> </ul>"},{"location":"CONTRIBUTING/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commit format:</p> <pre><code>type(scope): brief description\n\nMore detailed explanation if needed\n\n- Use present tense (\"Add feature\" not \"Added feature\")\n- Limit first line to 72 characters\n- Reference issues and pull requests liberally\n</code></pre> <p>Examples: - <code>feat(extractor): add support for custom prompt templates</code> - <code>fix(parser): handle malformed XML gracefully</code> - <code>docs(readme): update installation instructions</code></p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":""},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npython -m pytest tests/\n\n# Run specific test file\npython -m pytest tests/test_extractor.py\n\n# Run with coverage\npython -m pytest tests/ --cov=src/chemistry_llm --cov-report=html\n</code></pre>"},{"location":"CONTRIBUTING/#writing-tests","title":"Writing Tests","text":"<ul> <li>Write tests for all new functionality</li> <li>Maintain or improve test coverage</li> <li>Use descriptive test names</li> <li>Include both positive and negative test cases</li> <li>Mock external dependencies (models, APIs)</li> </ul>"},{"location":"CONTRIBUTING/#test-structure","title":"Test Structure","text":"<pre><code>class TestNewFeature:\n    \"\"\"Test cases for new feature\"\"\"\n\n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Provide sample data for tests\"\"\"\n        return {...}\n\n    def test_feature_success_case(self, sample_data):\n        \"\"\"Test successful execution\"\"\"\n        # Test implementation\n\n    def test_feature_error_handling(self):\n        \"\"\"Test error handling\"\"\"\n        # Test implementation\n</code></pre>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":""},{"location":"CONTRIBUTING/#python-style-guidelines","title":"Python Style Guidelines","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 100 characters (not 79)</li> <li>Use double quotes for strings</li> <li>Use type hints where appropriate</li> <li>Use docstrings for all public functions and classes</li> </ul>"},{"location":"CONTRIBUTING/#code-formatting","title":"Code Formatting","text":"<p>We use automated formatting tools:</p> <pre><code># Format code\nblack src/ tests/ examples/\nisort src/ tests/ examples/\n\n# Check formatting\nblack --check src/ tests/ examples/\nisort --check-only src/ tests/ examples/\n\n# Lint code\nflake8 src/ tests/ examples/\n\n# Type checking\nmypy src/\n</code></pre>"},{"location":"CONTRIBUTING/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks are configured to run automatically:</p> <ul> <li>Black (code formatting)</li> <li>isort (import sorting)</li> <li>flake8 (linting)</li> <li>mypy (type checking)</li> </ul>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":""},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Update your branch <code>bash    git checkout main    git pull upstream main    git checkout your-feature-branch    git rebase main</code></p> </li> <li> <p>Run tests and formatting <code>bash    black src/ tests/    isort src/ tests/    flake8 src/ tests/    python -m pytest tests/</code></p> </li> <li> <p>Create pull request</p> </li> <li>Use a clear, descriptive title</li> <li>Include a detailed description</li> <li>Reference related issues</li> <li>Add screenshots if applicable</li> <li>Request reviews from maintainers</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-template","title":"Pull Request Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] All tests pass\n- [ ] Added new tests for functionality\n- [ ] Manual testing completed\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] No breaking changes (or documented)\n</code></pre>"},{"location":"CONTRIBUTING/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>At least one maintainer review required</li> <li>Address all review comments</li> <li>Squash commits if requested</li> <li>Merge after approval</li> </ol>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":""},{"location":"CONTRIBUTING/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def analyze_procedure(self, procedure_text: str, return_raw: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyze a chemical procedure and extract reaction information.\n\n    Args:\n        procedure_text: The chemical procedure text to analyze\n        return_raw: Whether to include raw model output in results\n\n    Returns:\n        Dictionary containing extracted reaction data and metadata\n\n    Raises:\n        ValueError: If procedure_text is empty or invalid\n        ModelNotLoadedException: If model hasn't been loaded\n\n    Example:\n        &gt;&gt;&gt; extractor = ChemistryReactionExtractor(\"./model\")\n        &gt;&gt;&gt; result = extractor.analyze_procedure(\"Add 5g NaCl to water...\")\n        &gt;&gt;&gt; print(result[\"extracted_data\"])\n    \"\"\"\n</code></pre>"},{"location":"INSTALLATION/","title":"Installation &amp; Setup Guide","text":"<p>Complete installation instructions for RxNExtract across different platforms and use cases.</p>"},{"location":"INSTALLATION/#quick-installation","title":"\ud83d\ude80 Quick Installation","text":""},{"location":"INSTALLATION/#recommended-pypi-installation","title":"Recommended: PyPI Installation","text":"<pre><code>pip install rxnextract\n</code></pre> <p>That's it! For most users, this one command provides everything needed to start extracting chemical reaction information.</p>"},{"location":"INSTALLATION/#system-requirements","title":"\ud83d\udccb System Requirements","text":""},{"location":"INSTALLATION/#minimum-requirements","title":"Minimum Requirements","text":"Component Specification Operating System Linux (Ubuntu 18.04+), macOS (10.14+), Windows 10+ Python 3.8 or higher RAM 8GB Storage 20GB available space CPU 4 cores, x64 architecture Internet Required for model downloads"},{"location":"INSTALLATION/#recommended-requirements","title":"Recommended Requirements","text":"Component Specification Python 3.9+ RAM 16GB+ GPU CUDA-compatible GPU with 12GB+ VRAM Storage 50GB+ SSD storage CPU 8+ cores"},{"location":"INSTALLATION/#gpu-requirements-optional-but-recommended","title":"GPU Requirements (Optional but Recommended)","text":"<ul> <li>NVIDIA GPU with CUDA Compute Capability 6.0+</li> <li>CUDA version 11.0+ </li> <li>cuDNN version 8.0+</li> <li>GPU Memory: Minimum 4GB, Recommended 12GB+</li> </ul> <p>Note: All requirements are for inference only. Model fine-tuning requires significantly more resources.</p>"},{"location":"INSTALLATION/#installation-methods","title":"\ud83d\udce6 Installation Methods","text":""},{"location":"INSTALLATION/#method-1-pypi-installation-recommended","title":"Method 1: PyPI Installation (Recommended)","text":""},{"location":"INSTALLATION/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install rxnextract\n</code></pre>"},{"location":"INSTALLATION/#installation-variants","title":"Installation Variants","text":"<pre><code># CPU-only version (smaller download)\npip install rxnextract[cpu]\n\n# GPU-accelerated version\npip install rxnextract[gpu]\n\n# Complete installation with all optional dependencies\npip install rxnextract[full]\n\n# Development installation with testing tools\npip install rxnextract[dev]\n\n# Documentation building tools\npip install rxnextract[docs]\n</code></pre>"},{"location":"INSTALLATION/#virtual-environment-setup-recommended","title":"Virtual Environment Setup (Recommended)","text":"<pre><code># Create virtual environment\npython -m venv rxnextract-env\n\n# Activate virtual environment\n# Linux/macOS:\nsource rxnextract-env/bin/activate\n# Windows:\nrxnextract-env\\Scripts\\activate\n\n# Install RxNExtract\npip install rxnextract[full]\n</code></pre>"},{"location":"INSTALLATION/#method-2-docker-installation","title":"Method 2: Docker Installation","text":""},{"location":"INSTALLATION/#quick-start-with-docker","title":"Quick Start with Docker","text":"<pre><code># Pull latest stable image\ndocker pull chemplusx/rxnextract:latest\n\n# Run interactive mode\ndocker run -it --gpus all chemplusx/rxnextract:latest\n\n# Run with mounted data directory\ndocker run -v /path/to/your/data:/app/data \\\n  chemplusx/rxnextract:latest \\\n  --input /app/data/procedures.txt \\\n  --output /app/data/results.json\n</code></pre>"},{"location":"INSTALLATION/#docker-variants","title":"Docker Variants","text":"<pre><code># CPU-only version (smaller image)\ndocker pull chemplusx/rxnextract:latest-cpu\n\n# GPU-accelerated version\ndocker pull chemplusx/rxnextract:latest-gpu\n\n# Development version with additional tools\ndocker pull chemplusx/rxnextract:dev\n\n# Specific version\ndocker pull chemplusx/rxnextract:v1.2.0\n</code></pre>"},{"location":"INSTALLATION/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\nservices:\n  rxnextract:\n    image: chemplusx/rxnextract:latest\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    environment:\n      - CUDA_VISIBLE_DEVICES=0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> <p>Run:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"INSTALLATION/#method-3-from-source-development","title":"Method 3: From Source (Development)","text":""},{"location":"INSTALLATION/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone repository\ngit clone https://github.com/chemplusx/RxNExtract.git\ncd RxNExtract\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or\nvenv\\Scripts\\activate  # Windows\n\n# Install development dependencies\npip install -r requirements.txt\npip install -r requirements-dev.txt\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"INSTALLATION/#development-setup-with-pre-commit-hooks","title":"Development Setup with Pre-commit Hooks","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run hooks on all files (optional)\npre-commit run --all-files\n</code></pre>"},{"location":"INSTALLATION/#model-setup","title":"\ud83e\udd16 Model Setup","text":""},{"location":"INSTALLATION/#automatic-model-download-recommended","title":"Automatic Model Download (Recommended)","text":"<p>Models are automatically downloaded on first use:</p> <pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# This will automatically download the default model\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n</code></pre>"},{"location":"INSTALLATION/#pre-download-models","title":"Pre-download Models","text":"<pre><code># Download default model\npython -c \"from chemistry_llm import ChemistryReactionExtractor; ChemistryReactionExtractor.download_default_model()\"\n\n# Download specific model\npython -c \"from chemistry_llm import ChemistryReactionExtractor; ChemistryReactionExtractor.from_pretrained('chemplusx/rxnextract-base')\"\n</code></pre>"},{"location":"INSTALLATION/#huggingface-integration","title":"HuggingFace Integration","text":""},{"location":"INSTALLATION/#available-models","title":"Available Models","text":"Model Description Size Performance <code>chemplusx/rxnextract-base</code> Base fine-tuned model 7B params Good <code>chemplusx/rxnextract-complete</code> Complete framework model 7B params Best <code>chemplusx/rxnextract-fast</code> Optimized for speed 1B params Fast"},{"location":"INSTALLATION/#manual-model-download","title":"Manual Model Download","text":"<pre><code># Using git lfs\ngit lfs install\ngit clone https://huggingface.co/chemplusx/rxnextract-complete\n\n# Using HuggingFace Hub\npip install huggingface_hub\npython -c \"\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id='chemplusx/rxnextract-complete', local_dir='./models/complete')\n\"\n</code></pre>"},{"location":"INSTALLATION/#local-model-setup","title":"Local Model Setup","text":"<p>If you have your own fine-tuned model:</p> <pre><code>your-model-path/\n\u251c\u2500\u2500 adapter_config.json\n\u251c\u2500\u2500 adapter_model.bin  # or adapter_model.safetensors\n\u251c\u2500\u2500 tokenizer.json\n\u251c\u2500\u2500 tokenizer_config.json\n\u251c\u2500\u2500 special_tokens_map.json\n\u2514\u2500\u2500 tokenizer.model  # if using SentencePiece\n</code></pre>"},{"location":"INSTALLATION/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"INSTALLATION/#environment-variables","title":"Environment Variables","text":"<pre><code># Optional environment variables\nexport RXNEXTRACT_MODEL_PATH=\"/path/to/your/model\"\nexport RXNEXTRACT_CACHE_DIR=\"./cache\"\nexport RXNEXTRACT_DEVICE=\"cuda\"  # or \"cpu\", \"auto\"\nexport RXNEXTRACT_LOG_LEVEL=\"INFO\"\nexport HUGGINGFACE_HUB_CACHE=\"./hf_cache\"\n</code></pre>"},{"location":"INSTALLATION/#configuration-file","title":"Configuration File","text":"<p>Create <code>config/config.yaml</code>:</p> <pre><code>model:\n  default_model: \"chemplusx/rxnextract-complete\"\n  device: \"auto\"  # auto, cpu, cuda\n  max_length: 512\n  temperature: 0.1\n  top_p: 0.95\n  quantization:\n    load_in_4bit: true\n    bnb_4bit_quant_type: \"nf4\"\n    bnb_4bit_compute_dtype: \"float16\"\n\npaths:\n  model_cache: \"./cache/models\"\n  output_dir: \"./outputs\"\n  log_dir: \"./logs\"\n\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file: true\n\nanalysis:\n  error_analysis:\n    include_cot_failures: true\n    categorize_by_complexity: true\n\n  ablation_study:\n    sample_size: 1000\n    stratified_sampling: true\n\n  statistical_analysis:\n    significance_level: 0.05\n    confidence_level: 0.95\n    bootstrap_iterations: 1000\n\noutput:\n  format: \"json\"  # json, xml, yaml\n  include_raw: false\n  include_confidence: true\n  pretty_print: true\n</code></pre>"},{"location":"INSTALLATION/#hardware-optimization","title":"\ud83d\udd27 Hardware Optimization","text":""},{"location":"INSTALLATION/#gpu-setup","title":"GPU Setup","text":""},{"location":"INSTALLATION/#nvidia-gpu-optimization","title":"NVIDIA GPU Optimization","text":"<pre><code># Check CUDA availability\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA devices: {torch.cuda.device_count()}')\"\n\n# Install CUDA-specific PyTorch (if needed)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"INSTALLATION/#memory-optimization","title":"Memory Optimization","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# Use 4-bit quantization for lower memory usage\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    load_in_4bit=True,\n    device=\"cuda\"\n)\n\n# Or use CPU if GPU memory is insufficient\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    device=\"cpu\"\n)\n</code></pre>"},{"location":"INSTALLATION/#cpu-optimization","title":"CPU Optimization","text":"<pre><code>import torch\n\n# Set number of CPU threads\ntorch.set_num_threads(8)\n\n# Use CPU-optimized model loading\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    device=\"cpu\",\n    torch_dtype=torch.float32  # Use float32 for better CPU performance\n)\n</code></pre>"},{"location":"INSTALLATION/#verification","title":"\u2705 Verification","text":""},{"location":"INSTALLATION/#test-installation","title":"Test Installation","text":"<pre><code># Basic functionality test\npython -c \"\nfrom chemistry_llm import ChemistryReactionExtractor\nprint('\u2713 RxNExtract imported successfully')\n\n# Test model loading\ntry:\n    extractor = ChemistryReactionExtractor.from_pretrained('chemplusx/rxnextract-complete')\n    print('\u2713 Model loaded successfully')\nexcept Exception as e:\n    print(f'\u2717 Model loading failed: {e}')\n\"\n</code></pre>"},{"location":"INSTALLATION/#run-example-extraction","title":"Run Example Extraction","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# Test basic extraction\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\nprocedure = \"Add 5g NaCl to 100mL water and stir for 30 minutes.\"\n\ntry:\n    result = extractor.analyze_procedure(procedure)\n    print(\"\u2713 Extraction test passed\")\n    print(f\"Found {len(result['extracted_data']['reactants'])} reactants\")\nexcept Exception as e:\n    print(f\"\u2717 Extraction test failed: {e}\")\n</code></pre>"},{"location":"INSTALLATION/#run-test-suite","title":"Run Test Suite","text":"<pre><code># Install test dependencies\npip install pytest pytest-cov\n\n# Run basic tests\npython -m pytest tests/test_basic.py -v\n\n# Run full test suite\npython -m pytest tests/ -v\n\n# Run with coverage\npython -m pytest tests/ --cov=chemistry_llm --cov-report=html\n</code></pre>"},{"location":"INSTALLATION/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"INSTALLATION/#common-issues","title":"Common Issues","text":""},{"location":"INSTALLATION/#issue-cuda-out-of-memory","title":"Issue: CUDA out of memory","text":"<p>Solution:</p> <pre><code># Use 4-bit quantization\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    load_in_4bit=True\n)\n\n# Or use CPU\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    device=\"cpu\"\n)\n</code></pre>"},{"location":"INSTALLATION/#issue-model-download-fails","title":"Issue: Model download fails","text":"<p>Solution:</p> <pre><code># Set HuggingFace cache directory\nexport HF_HOME=\"./hf_cache\"\n\n# Use manual download\nhuggingface-cli download chemplusx/rxnextract-complete\n\n# Check internet connection and try again\nping huggingface.co\n</code></pre>"},{"location":"INSTALLATION/#issue-import-errors","title":"Issue: Import errors","text":"<p>Solution:</p> <pre><code># Ensure all dependencies are installed\npip install --upgrade rxnextract[full]\n\n# Check Python version\npython --version  # Should be 3.8+\n\n# Check installation\npip show rxnextract\n</code></pre>"},{"location":"INSTALLATION/#issue-slow-performance-on-cpu","title":"Issue: Slow performance on CPU","text":"<p>Solution:</p> <pre><code>import torch\n\n# Optimize CPU performance\ntorch.set_num_threads(min(8, torch.get_num_threads()))\n\n# Use smaller model for faster inference\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-fast\"\n)\n</code></pre>"},{"location":"INSTALLATION/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"INSTALLATION/#macos-specific","title":"macOS-Specific","text":"<pre><code># Install Xcode command line tools\nxcode-select --install\n\n# Use Homebrew for dependencies\nbrew install python@3.9\n\n# For Apple Silicon Macs, ensure compatible PyTorch\npip install torch torchvision torchaudio\n</code></pre>"},{"location":"INSTALLATION/#linux-specific","title":"Linux-Specific","text":"<pre><code># Install system dependencies\nsudo apt-get update\nsudo apt-get install python3-dev build-essential\n\n# For GPU support\nsudo apt-get install nvidia-driver-470 nvidia-cuda-toolkit\n</code></pre>"},{"location":"INSTALLATION/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the FAQ: docs/FAQ.md</li> <li>Search existing issues: GitHub Issues</li> <li>Create a new issue: Include your system info, Python version, and error messages</li> <li>Join discussions: GitHub Discussions</li> <li>Contact support: support@rxnextract.org</li> </ol>"},{"location":"INSTALLATION/#updating","title":"\ud83d\udd04 Updating","text":""},{"location":"INSTALLATION/#update-via-pypi","title":"Update via PyPI","text":"<pre><code>pip install --upgrade rxnextract\n</code></pre>"},{"location":"INSTALLATION/#update-docker-images","title":"Update Docker Images","text":"<pre><code>docker pull chemplusx/rxnextract:latest\n</code></pre>"},{"location":"INSTALLATION/#update-from-source","title":"Update from Source","text":"<pre><code>cd RxNExtract\ngit pull origin main\npip install -e .\n</code></pre>"},{"location":"INSTALLATION/#security-considerations","title":"\ud83d\udd12 Security Considerations","text":"<ul> <li>Model files: Downloaded models are cached locally and verified for integrity</li> <li>Network access: Required only for initial model download</li> <li>Data privacy: All processing is done locally; no data is sent to external servers</li> <li>Dependencies: All dependencies are from trusted sources (PyPI)</li> </ul> <p>For enterprise deployments, consider: - Using local model storage - Network-isolated environments - Custom Docker images with pre-downloaded models</p> <p>Next Steps: After installation, see the Usage Guide for detailed examples and API documentation.</p>"},{"location":"USAGE/","title":"Usage Guide &amp; Examples","text":"<p>Comprehensive guide to using RxNExtract for chemical reaction extraction, from basic usage to advanced analysis.</p>"},{"location":"USAGE/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"USAGE/#basic-python-usage","title":"Basic Python Usage","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# Initialize extractor with pre-trained model\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n\n# Analyze a chemical procedure\nprocedure = \"\"\"\nAdd 2.5 g of benzoic acid to 50 mL of ethanol. \nHeat the mixture to reflux for 4 hours.\nCool and filter to obtain the product.\n\"\"\"\n\n# Extract reaction information\nresults = extractor.analyze_procedure(procedure)\n\n# Access structured data\ndata = results['extracted_data']\nprint(\"Reactants:\", data['reactants'])\nprint(\"Products:\", data['products'])\nprint(\"Conditions:\", data['conditions'])\nprint(\"Reagents:\", data['reagents'])\n</code></pre>"},{"location":"USAGE/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Interactive mode for real-time analysis\nrxnextract --interactive\n\n# Analyze single procedure\nrxnextract --procedure \"Add 5g NaCl to 100mL water and stir for 30 minutes\"\n\n# Batch processing from file\nrxnextract --input procedures.txt --output results.json\n\n# With custom model\nrxnextract --model-path ./custom-model --input data.txt --output results.json\n</code></pre>"},{"location":"USAGE/#programmatic-api","title":"\ud83d\udcd6 Programmatic API","text":""},{"location":"USAGE/#chemistryreactionextractor-class","title":"ChemistryReactionExtractor Class","text":""},{"location":"USAGE/#initialization-options","title":"Initialization Options","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\n\n# Option 1: Load from HuggingFace Hub (recommended)\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    device=\"cuda\",  # \"cpu\", \"cuda\", \"auto\"\n    load_in_4bit=True,  # Memory optimization\n    temperature=0.1,\n    max_length=512\n)\n\n# Option 2: Load from local model path\nextractor = ChemistryReactionExtractor(\n    model_path=\"./path/to/local/model\",\n    device=\"auto\",\n    config={\n        \"temperature\": 0.1,\n        \"top_p\": 0.95,\n        \"max_new_tokens\": 512\n    }\n)\n\n# Option 3: Advanced configuration\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    torch_dtype=\"float16\",\n    device_map=\"auto\",  # Automatic device assignment\n    quantization_config={\n        \"load_in_4bit\": True,\n        \"bnb_4bit_quant_type\": \"nf4\",\n        \"bnb_4bit_compute_dtype\": \"float16\"\n    }\n)\n</code></pre>"},{"location":"USAGE/#core-methods","title":"Core Methods","text":""},{"location":"USAGE/#analyze_procedureprocedure_text-return_rawfalse-kwargs","title":"<code>analyze_procedure(procedure_text, return_raw=False, **kwargs)</code>","text":"<p>Main method for extracting reaction information.</p> <pre><code># Basic usage\nresults = extractor.analyze_procedure(procedure_text)\n\n# With raw output included\nresults = extractor.analyze_procedure(\n    procedure_text, \n    return_raw=True,\n    temperature=0.05  # Override default temperature\n)\n\n# Return structure\n{\n    'extracted_data': {\n        'reactants': [{'name': 'benzoic acid', 'amount': '2.5 g', ...}],\n        'products': [{'name': 'product', 'yield': '84%', ...}],\n        'reagents': [{'name': 'HCl', 'amount': '10 mL', ...}],\n        'solvents': [{'name': 'ethanol', 'amount': '50 mL', ...}],\n        'conditions': {\n            'temperature': 'reflux',\n            'time': '4 hours',\n            'atmosphere': 'ambient'\n        },\n        'workup': ['Cool', 'filter', 'wash with cold water']\n    },\n    'confidence': 0.89,\n    'processing_time': 2.3,\n    'raw_output': '...'  # If return_raw=True\n}\n</code></pre>"},{"location":"USAGE/#extract_reactionprocedure_text-generation_params","title":"<code>extract_reaction(procedure_text, **generation_params)</code>","text":"<p>Low-level extraction method for advanced users.</p> <pre><code>raw_output = extractor.extract_reaction(\n    procedure_text,\n    temperature=0.1,\n    max_new_tokens=512,\n    do_sample=True,\n    top_p=0.95\n)\n</code></pre>"},{"location":"USAGE/#batch_analyzeprocedures_list-batch_size8-show_progresstrue","title":"<code>batch_analyze(procedures_list, batch_size=8, show_progress=True)</code>","text":"<p>Efficient batch processing with progress tracking.</p> <pre><code>procedures = [\n    \"Mix compound A with solvent B...\",\n    \"Heat the reaction mixture to 150\u00b0C...\",\n    \"Add catalyst C slowly while stirring...\"\n]\n\n# Batch processing\nresults = extractor.batch_analyze(\n    procedures,\n    batch_size=4,\n    show_progress=True\n)\n\n# Results is a list of analysis results\nfor i, result in enumerate(results):\n    print(f\"Procedure {i+1}: {len(result['extracted_data']['reactants'])} reactants\")\n</code></pre>"},{"location":"USAGE/#command-line-interface_1","title":"\ud83d\udda5\ufe0f Command Line Interface","text":""},{"location":"USAGE/#interactive-mode","title":"Interactive Mode","text":"<p>Start an interactive session for real-time analysis:</p> <pre><code>rxnextract --interactive\n</code></pre> <p>Features: - Real-time procedure input and analysis - Formatted output display with syntax highlighting - Session history and recall - Error handling and recovery - Multi-line input support</p> <p>Interactive commands:</p> <pre><code>&gt; analyze: Add 5g NaCl to water\n&gt; history: Show previous analyses\n&gt; save results.json: Save current session\n&gt; load previous.json: Load previous session\n&gt; config: Show current configuration\n&gt; help: Show available commands\n&gt; quit: Exit interactive mode\n</code></pre>"},{"location":"USAGE/#batch-processing","title":"Batch Processing","text":"<p>Process multiple procedures from various input formats:</p> <pre><code># Basic batch processing\nrxnextract --input procedures.txt --output results.json\n\n# With progress bar and custom batch size\nrxnextract --input procedures.txt --output results.json --batch-size 16 --progress\n\n# Specify model and device\nrxnextract --model chemplusx/rxnextract-complete --device cuda --input data.txt\n\n# With confidence filtering\nrxnextract --input procedures.txt --output results.json --min-confidence 0.8\n\n# Custom output format\nrxnextract --input procedures.txt --output results.xml --format xml\n</code></pre>"},{"location":"USAGE/#input-file-formats","title":"Input File Formats","text":"<p>Text file (<code>.txt</code>) - One procedure per line:</p> <pre><code>Add 5g NaCl to 100mL water and stir for 30 minutes.\nReflux the mixture of benzene and AlCl3 for 2 hours at 80\u00b0C.\nCool the solution to room temperature and filter the precipitate.\n</code></pre> <p>JSON file (<code>.json</code>) - Structured input:</p> <pre><code>[\n    {\n        \"id\": \"proc_001\",\n        \"procedure\": \"Add 5g NaCl to 100mL water and stir for 30 minutes.\",\n        \"metadata\": {\"source\": \"paper_1\", \"page\": 15}\n    },\n    {\n        \"id\": \"proc_002\", \n        \"procedure\": \"Reflux the mixture of benzene and AlCl3 for 2 hours at 80\u00b0C.\",\n        \"metadata\": {\"source\": \"paper_2\", \"page\": 8}\n    }\n]\n</code></pre> <p>CSV file (<code>.csv</code>) - Tabular format:</p> <pre><code>id,procedure,source,page\nproc_001,\"Add 5g NaCl to 100mL water and stir for 30 minutes.\",paper_1,15\nproc_002,\"Reflux the mixture of benzene and AlCl3 for 2 hours at 80\u00b0C.\",paper_2,8\n</code></pre>"},{"location":"USAGE/#advanced-cli-options","title":"Advanced CLI Options","text":"<pre><code># Full list of CLI options\nrxnextract --help\n\n# Memory optimization\nrxnextract --input data.txt --load-in-4bit --max-memory 8GB\n\n# Custom configuration\nrxnextract --config custom_config.yaml --input procedures.txt\n\n# Parallel processing\nrxnextract --input data.txt --workers 4 --output results.json\n\n# Verbose logging\nrxnextract --input data.txt --log-level DEBUG --log-file extraction.log\n</code></pre>"},{"location":"USAGE/#advanced-usage-examples","title":"\ud83d\udcca Advanced Usage Examples","text":""},{"location":"USAGE/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\nfrom chemistry_llm.utils import setup_logging\n\n# Setup custom logging\nsetup_logging(level=\"INFO\", log_file=\"extraction.log\")\n\n# Custom configuration\nconfig = {\n    \"model\": {\n        \"temperature\": 0.05,\n        \"top_p\": 0.9,\n        \"max_new_tokens\": 1024,\n        \"repetition_penalty\": 1.1\n    },\n    \"prompts\": {\n        \"use_cot\": True,\n        \"cot_steps\": [\n            \"Identify all chemical compounds\",\n            \"Determine reaction roles\",\n            \"Extract reaction conditions\",\n            \"Identify products and yields\"\n        ]\n    },\n    \"output\": {\n        \"include_confidence\": True,\n        \"include_reasoning\": True,\n        \"xml_pretty_print\": True\n    }\n}\n\n# Initialize with custom config\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    config=config\n)\n</code></pre>"},{"location":"USAGE/#multi-document-processing","title":"Multi-Document Processing","text":"<pre><code>import json\nfrom pathlib import Path\nfrom chemistry_llm import ChemistryReactionExtractor\n\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n\ndef process_documents(input_dir, output_dir):\n    \"\"\"Process all text files in a directory\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    for txt_file in input_path.glob(\"*.txt\"):\n        print(f\"Processing {txt_file.name}...\")\n\n        with open(txt_file, 'r', encoding='utf-8') as f:\n            procedures = [line.strip() for line in f if line.strip()]\n\n        # Batch process\n        results = extractor.batch_analyze(procedures, batch_size=8)\n\n        # Save results\n        output_file = output_path / f\"{txt_file.stem}_results.json\"\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, indent=2, ensure_ascii=False)\n\n        print(f\"Saved {len(results)} results to {output_file}\")\n\n# Process all documents\nprocess_documents(\"./input_docs\", \"./output_results\")\n</code></pre>"},{"location":"USAGE/#integration-with-scientific-libraries","title":"Integration with Scientific Libraries","text":"<pre><code>import pandas as pd\nfrom chemistry_llm import ChemistryReactionExtractor\nfrom rdkit import Chem\nfrom rdkit.Chem import Draw\n\n# Initialize extractor\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n\ndef analyze_and_visualize(procedures_df):\n    \"\"\"Analyze procedures and create molecular visualizations\"\"\"\n    results = []\n\n    for idx, row in procedures_df.iterrows():\n        # Extract reaction information\n        result = extractor.analyze_procedure(row['procedure'])\n\n        # Process reactants for visualization\n        reactant_smiles = []\n        for reactant in result['extracted_data']['reactants']:\n            if 'smiles' in reactant:\n                reactant_smiles.append(reactant['smiles'])\n\n        # Create molecular visualization\n        if reactant_smiles:\n            mols = [Chem.MolFromSmiles(smiles) for smiles in reactant_smiles]\n            img = Draw.MolsToGridImage(mols, molsPerRow=3)\n            img.save(f\"reaction_{idx}_molecules.png\")\n\n        # Store results\n        results.append({\n            'procedure_id': row['id'],\n            'reactants_count': len(result['extracted_data']['reactants']),\n            'products_count': len(result['extracted_data']['products']),\n            'confidence': result['confidence'],\n            'extraction_result': result\n        })\n\n    return pd.DataFrame(results)\n\n# Example usage\nprocedures_df = pd.read_csv(\"procedures.csv\")\nanalysis_results = analyze_and_visualize(procedures_df)\nanalysis_results.to_csv(\"analysis_summary.csv\", index=False)\n</code></pre>"},{"location":"USAGE/#error-handling-and-robustness","title":"Error Handling and Robustness","text":"<pre><code>from chemistry_llm import ChemistryReactionExtractor\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef robust_extraction(procedures, max_retries=3):\n    \"\"\"Robust extraction with error handling and retries\"\"\"\n    extractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n    results = []\n\n    for i, procedure in enumerate(procedures):\n        success = False\n        retries = 0\n\n        while not success and retries &lt; max_retries:\n            try:\n                result = extractor.analyze_procedure(procedure)\n\n                # Validate result\n                if result['confidence'] &lt; 0.5:\n                    logger.warning(f\"Low confidence ({result['confidence']:.2f}) for procedure {i}\")\n\n                results.append({\n                    'procedure_index': i,\n                    'success': True,\n                    'result': result,\n                    'retries_used': retries\n                })\n                success = True\n\n            except Exception as e:\n                retries += 1\n                logger.error(f\"Extraction failed for procedure {i} (attempt {retries}): {e}\")\n\n                if retries &gt;= max_retries:\n                    results.append({\n                        'procedure_index': i,\n                        'success': False,\n                        'error': str(e),\n                        'retries_used': retries\n                    })\n\n    return results\n</code></pre>"},{"location":"USAGE/#analysis-evaluation-framework","title":"\ud83d\udd0d Analysis &amp; Evaluation Framework","text":""},{"location":"USAGE/#error-analysis","title":"Error Analysis","text":"<p>Systematic analysis of extraction errors across different categories:</p> <pre><code>from chemistry_llm.analysis import ErrorAnalyzer\n\n# Initialize error analyzer\nerror_analyzer = ErrorAnalyzer()\n\n# Load your predictions and ground truth\npredictions = load_predictions(\"model_predictions.json\")\nground_truth = load_ground_truth(\"ground_truth.json\")\n\n# Comprehensive error analysis\nerror_results = error_analyzer.analyze_prediction_errors(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    method_name=\"RxNExtract-Complete\"\n)\n\n# Analyze specific error categories\nentity_errors = error_analyzer.analyze_entity_errors(predictions, ground_truth)\nrole_errors = error_analyzer.analyze_role_classification_errors(predictions, ground_truth)\ncondition_errors = error_analyzer.analyze_condition_extraction_errors(predictions, ground_truth)\n\n# Chain-of-Thought failure analysis\ncot_failures = error_analyzer.analyze_cot_failures(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    raw_outputs=raw_model_outputs\n)\n\n# Generate comprehensive error report\nreport = error_analyzer.generate_error_report(\n    error_results, \n    output_file=\"comprehensive_error_analysis.txt\"\n)\n\nprint(\"Error Analysis Summary:\")\nprint(f\"Total errors analyzed: {error_results.total_errors}\")\nprint(f\"Entity recognition errors: {error_results.entity_errors}\")\nprint(f\"Role classification errors: {error_results.role_errors}\")\nprint(f\"Condition extraction errors: {error_results.condition_errors}\")\n</code></pre>"},{"location":"USAGE/#ablation-studies","title":"Ablation Studies","text":"<p>Systematic component-level performance analysis:</p> <pre><code>from chemistry_llm.analysis import AblationStudy\n\n# Initialize ablation study\nablation = AblationStudy(model_path=\"./model\")\n\n# Run complete ablation study\nstudy_results = ablation.run_complete_study(\n    test_data=test_procedures,\n    ground_truth=ground_truth,\n    sample_size=1000,\n    stratified=True,  # Stratify by reaction complexity\n    random_state=42\n)\n\n# Analyze dynamic prompt components\ndynamic_analysis = ablation.analyze_dynamic_prompt_components(\n    test_sample=test_procedures[:100],\n    truth_sample=ground_truth[:100]\n)\n\n# Component contribution analysis\ncomponent_contributions = ablation.analyze_component_contributions(\n    study_results\n)\n\n# Generate ablation report\nreport = ablation.generate_ablation_report(\n    study_results, \n    output_file=\"ablation_study_report.txt\"\n)\n\n# Export results for further analysis\ndf = ablation.export_results_to_csv(study_results, \"ablation_results.csv\")\n\nprint(\"Ablation Study Results:\")\nfor config, metrics in study_results.items():\n    print(f\"{config}: CRA = {metrics.cra:.3f}, F1 = {metrics.entity_f1:.3f}\")\n</code></pre>"},{"location":"USAGE/#available-ablation-configurations","title":"Available Ablation Configurations","text":"<ul> <li>Direct Extraction: Basic extraction without enhancements</li> <li>Structured Output: XML-structured output format</li> <li>Meta Prompt: Enhanced prompt engineering</li> <li>Chain-of-Thought: Step-by-step reasoning</li> <li>CoT + Reflection: Chain-of-thought with self-reflection</li> <li>Self-Grounding: Entity validation and correction</li> <li>Complete Framework: All components combined</li> <li>Iterative Refinement: Multi-pass extraction</li> </ul>"},{"location":"USAGE/#statistical-analysis","title":"Statistical Analysis","text":"<p>Comprehensive statistical testing and significance analysis:</p> <pre><code>from chemistry_llm.analysis import StatisticalAnalyzer\n\n# Initialize statistical analyzer\nstats_analyzer = StatisticalAnalyzer()\n\n# Load results from different methods\nbaseline_results = load_results(\"baseline_predictions.json\")\nimproved_results = load_results(\"rxnextract_predictions.json\")\n\n# Pairwise method comparison\ncomparison = stats_analyzer.perform_pairwise_comparison(\n    method1_results=[r['cra'] for r in baseline_results],\n    method2_results=[r['cra'] for r in improved_results],\n    method1_name=\"Baseline\",\n    method2_name=\"RxNExtract\",\n    test_type=\"paired_t\"  # or \"wilcoxon\", \"mann_whitney\"\n)\n\nprint(f\"Statistical Comparison Results:\")\nprint(f\"p-value: {comparison['p_value']:.6f}\")\nprint(f\"Effect size (Cohen's d): {comparison['effect_size']:.3f}\")\nprint(f\"Statistically significant: {comparison['significant']}\")\n\n# McNemar's test for classification performance\nbaseline_correct = [is_correct(pred, truth) for pred, truth in zip(baseline_results, ground_truth)]\nimproved_correct = [is_correct(pred, truth) for pred, truth in zip(improved_results, ground_truth)]\n\nmcnemar_result = stats_analyzer.perform_mcnemar_test(\n    method1_correct=baseline_correct,\n    method2_correct=improved_correct,\n    method1_name=\"Baseline\",\n    method2_name=\"RxNExtract\"\n)\n\n# ANOVA for multiple method comparison\nmethods_data = {\n    'Baseline': [r['cra'] for r in baseline_results],\n    'CoT-Only': [r['cra'] for r in cot_results],\n    'RxNExtract': [r['cra'] for r in improved_results]\n}\n\nanova_results = stats_analyzer.perform_anova(\n    groups=methods_data,\n    post_hoc=True  # Include post-hoc pairwise comparisons\n)\n\n# Generate statistical report\nstats_report = stats_analyzer.generate_statistical_report(\n    {\n        'pairwise_comparisons': {'baseline_vs_rxnextract': comparison},\n        'mcnemar_tests': {'classification_performance': mcnemar_result},\n        'anova_results': anova_results\n    },\n    output_file=\"statistical_analysis_report.txt\"\n)\n</code></pre>"},{"location":"USAGE/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<p>Confidence calibration and uncertainty analysis:</p> <pre><code>from chemistry_llm.analysis import UncertaintyQuantifier\n\n# Initialize uncertainty quantifier\nuncertainty = UncertaintyQuantifier()\n\n# Extract confidence scores from predictions\nconfidences = [pred['confidence'] for pred in predictions]\naccuracies = [1.0 if is_correct(pred, truth) else 0.0 \n              for pred, truth in zip(predictions, ground_truth)]\n\n# Calculate calibration metrics\ncalibration_metrics = uncertainty.calculate_calibration_metrics(\n    confidences=confidences,\n    accuracies=accuracies,\n    n_bins=10\n)\n\nprint(f\"Calibration Metrics:\")\nprint(f\"Expected Calibration Error (ECE): {calibration_metrics.ece:.4f}\")\nprint(f\"Brier Score: {calibration_metrics.brier_score:.4f}\")\nprint(f\"Reliability: {calibration_metrics.reliability:.4f}\")\n\n# Perform temperature scaling for calibration\ncalibrated_probs, optimal_temperature = uncertainty.perform_temperature_scaling(\n    validation_logits=validation_logits,\n    validation_labels=validation_labels,\n    test_logits=test_logits\n)\n\nprint(f\"Optimal temperature: {optimal_temperature:.3f}\")\n\n# Confidence-stratified performance analysis\nconfidence_analysis = uncertainty.analyze_confidence_stratified_performance(\n    confidences=confidences,\n    accuracies=accuracies,\n    n_strata=5\n)\n\n# Generate reliability diagram\nreliability_fig = uncertainty.generate_reliability_diagram(\n    confidences=confidences,\n    accuracies=accuracies,\n    save_path=\"reliability_diagram.png\"\n)\n\n# Comprehensive uncertainty analysis\nuncertainty_results = uncertainty.analyze_prediction_uncertainty(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    confidence_threshold=0.8\n)\n</code></pre>"},{"location":"USAGE/#metrics-calculator","title":"Metrics Calculator","text":"<p>Comprehensive performance metrics calculation:</p> <pre><code>from chemistry_llm.analysis import MetricsCalculator\n\n# Initialize metrics calculator\nmetrics_calc = MetricsCalculator()\n\n# Calculate comprehensive metrics\ncomprehensive_metrics = metrics_calc.calculate_comprehensive_metrics(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nprint(\"Performance Metrics:\")\nprint(f\"Complete Reaction Accuracy (CRA): {comprehensive_metrics['cra']:.3f}\")\nprint(f\"Entity F1 Score: {comprehensive_metrics['entity_f1']:.3f}\")\nprint(f\"Role Classification Accuracy: {comprehensive_metrics['rca']:.3f}\")\nprint(f\"Condition Extraction F1: {comprehensive_metrics['condition_f1']:.3f}\")\n\n# Performance by reaction complexity\ncomplexity_labels = assign_complexity_labels(ground_truth)  # Your complexity assignment logic\ncomplexity_metrics = metrics_calc.analyze_performance_by_complexity(\n    predictions=predictions,\n    ground_truth=ground_truth,\n    complexity_labels=complexity_labels\n)\n\n# Error reduction analysis\nbaseline_metrics = calculate_baseline_metrics(baseline_predictions, ground_truth)\nerror_reduction = metrics_calc.calculate_error_reduction(\n    baseline_metrics=baseline_metrics,\n    improved_metrics=comprehensive_metrics\n)\n\nprint(f\"Error Reduction:\")\nprint(f\"Entity Recognition: {error_reduction['entity_recognition']:.1f}%\")\nprint(f\"Role Classification: {error_reduction['role_classification']:.1f}%\")\nprint(f\"Condition Extraction: {error_reduction['condition_extraction']:.1f}%\")\n\n# Export metrics summary\nmetrics_calc.export_metrics_summary(\n    comprehensive_metrics, \n    \"comprehensive_metrics_summary.json\"\n)\n</code></pre>"},{"location":"USAGE/#utility-functions-and-helpers","title":"\ud83d\udd27 Utility Functions and Helpers","text":""},{"location":"USAGE/#xml-parsing-and-validation","title":"XML Parsing and Validation","text":"<pre><code>from chemistry_llm.utils.xml_parser import parse_reaction_xml, validate_xml_structure\n\n# Parse extracted XML to structured data\nxml_output = \"\"\"\n&lt;reaction&gt;\n    &lt;reactants&gt;\n        &lt;reactant name=\"benzoic acid\" amount=\"2.5 g\"/&gt;\n    &lt;/reactants&gt;\n    &lt;products&gt;\n        &lt;product name=\"product\" yield=\"84%\"/&gt;\n    &lt;/products&gt;\n    &lt;conditions&gt;\n        &lt;temperature&gt;reflux&lt;/temperature&gt;\n        &lt;time&gt;4 hours&lt;/time&gt;\n    &lt;/conditions&gt;\n&lt;/reaction&gt;\n\"\"\"\n\n# Parse XML\nstructured_data = parse_reaction_xml(xml_output)\n\n# Validate XML structure\nis_valid, errors = validate_xml_structure(xml_output)\nif not is_valid:\n    print(\"XML validation errors:\", errors)\n</code></pre>"},{"location":"USAGE/#device-and-memory-management","title":"Device and Memory Management","text":"<pre><code>from chemistry_llm.utils.device_utils import get_optimal_device, get_memory_info\n\n# Automatic device selection\ndevice = get_optimal_device()\nprint(f\"Selected device: {device}\")\n\n# Memory information\nmemory_info = get_memory_info()\nprint(f\"Available GPU memory: {memory_info['gpu_memory_available']:.1f} GB\")\nprint(f\"Available system RAM: {memory_info['system_memory_available']:.1f} GB\")\n\n# Optimize model loading based on available memory\nif memory_info['gpu_memory_available'] &gt; 12:\n    load_in_4bit = False\n    torch_dtype = \"float16\"\nelif memory_info['gpu_memory_available'] &gt; 6:\n    load_in_4bit = True\n    torch_dtype = \"float16\"\nelse:\n    # Use CPU\n    device = \"cpu\"\n    torch_dtype = \"float32\"\n</code></pre>"},{"location":"USAGE/#custom-prompt-engineering","title":"Custom Prompt Engineering","text":"<pre><code>from chemistry_llm.core.prompt_builder import PromptBuilder\n\n# Create custom prompt builder\nprompt_builder = PromptBuilder(\n    use_cot=True,\n    cot_steps=[\n        \"Identify all chemical compounds mentioned\",\n        \"Determine the role of each compound\",\n        \"Extract reaction conditions\",\n        \"Identify products and yields\",\n        \"Structure the output in XML format\"\n    ]\n)\n\n# Build custom prompt\ncustom_prompt = prompt_builder.build_extraction_prompt(\n    procedure_text=\"Your procedure here\",\n    include_examples=True,\n    format_type=\"xml\"\n)\n\n# Use with extractor\nresult = extractor.extract_reaction(\n    procedure_text,\n    custom_prompt=custom_prompt\n)\n</code></pre>"},{"location":"USAGE/#output-formats-and-post-processing","title":"\ud83d\udcca Output Formats and Post-Processing","text":""},{"location":"USAGE/#structured-output-formats","title":"Structured Output Formats","text":"<pre><code># JSON output (default)\nresults = extractor.analyze_procedure(procedure, output_format=\"json\")\n\n# XML output\nresults = extractor.analyze_procedure(procedure, output_format=\"xml\")\n\n# YAML output\nresults = extractor.analyze_procedure(procedure, output_format=\"yaml\")\n\n# Custom structured output\nresults = extractor.analyze_procedure(\n    procedure,\n    output_format=\"structured\",\n    include_confidence=True,\n    include_reasoning=True\n)\n</code></pre>"},{"location":"USAGE/#post-processing-and-validation","title":"Post-Processing and Validation","text":"<pre><code>from chemistry_llm.utils.post_processing import validate_extraction, normalize_entities\n\ndef post_process_results(results):\n    \"\"\"Post-process extraction results for quality assurance\"\"\"\n\n    # Validate extraction completeness\n    validation_results = validate_extraction(results['extracted_data'])\n\n    # Normalize chemical entities\n    normalized_data = normalize_entities(results['extracted_data'])\n\n    # Check for common extraction errors\n    if validation_results['missing_reactants']:\n        print(\"Warning: No reactants found\")\n\n    if validation_results['missing_products']:\n        print(\"Warning: No products found\")\n\n    if results['confidence'] &lt; 0.7:\n        print(f\"Warning: Low confidence score ({results['confidence']:.2f})\")\n\n    return {\n        **results,\n        'extracted_data': normalized_data,\n        'validation': validation_results\n    }\n\n# Apply post-processing\nprocessed_results = post_process_results(raw_results)\n</code></pre>"},{"location":"USAGE/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"USAGE/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code># Display results with rich formatting in Jupyter\nfrom IPython.display import display, HTML, JSON\nimport pandas as pd\n\ndef display_extraction_results(results):\n    \"\"\"Display extraction results in Jupyter with rich formatting\"\"\"\n\n    # Display confidence and timing\n    print(f\"Confidence: {results['confidence']:.2f}\")\n    print(f\"Processing time: {results['processing_time']:.1f}s\")\n\n    # Display extracted entities as DataFrame\n    entities_data = []\n    for reactant in results['extracted_data']['reactants']:\n        entities_data.append({\n            'Type': 'Reactant',\n            'Name': reactant['name'],\n            'Amount': reactant.get('amount', 'N/A'),\n            'Role': 'Reactant'\n        })\n\n    for product in results['extracted_data']['products']:\n        entities_data.append({\n            'Type': 'Product',\n            'Name': product['name'],\n            'Amount': product.get('yield', 'N/A'),\n            'Role': 'Product'\n        })\n\n    df = pd.DataFrame(entities_data)\n    display(df)\n\n    # Display conditions\n    if results['extracted_data']['conditions']:\n        print(\"\\nReaction Conditions:\")\n        display(JSON(results['extracted_data']['conditions']))\n\n# Use in Jupyter\nresults = extractor.analyze_procedure(procedure)\ndisplay_extraction_results(results)\n</code></pre>"},{"location":"USAGE/#web-api-integration","title":"Web API Integration","text":"<pre><code>from flask import Flask, request, jsonify\nfrom chemistry_llm import ChemistryReactionExtractor\n\napp = Flask(__name__)\nextractor = ChemistryReactionExtractor.from_pretrained(\"chemplusx/rxnextract-complete\")\n\n@app.route('/extract', methods=['POST'])\ndef extract_reaction():\n    \"\"\"Web API endpoint for reaction extraction\"\"\"\n    try:\n        data = request.get_json()\n        procedure = data.get('procedure', '')\n\n        if not procedure:\n            return jsonify({'error': 'No procedure provided'}), 400\n\n        # Extract reaction information\n        results = extractor.analyze_procedure(procedure)\n\n        return jsonify({\n            'success': True,\n            'results': results\n        })\n\n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e)\n        }), 500\n\n@app.route('/batch_extract', methods=['POST'])\ndef batch_extract():\n    \"\"\"Batch extraction endpoint\"\"\"\n    try:\n        data = request.get_json()\n        procedures = data.get('procedures', [])\n\n        if not procedures:\n            return jsonify({'error': 'No procedures provided'}), 400\n\n        # Batch processing\n        results = extractor.batch_analyze(procedures)\n\n        return jsonify({\n            'success': True,\n            'count': len(results),\n            'results': results\n        })\n\n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e)\n        }), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n</code></pre>"},{"location":"USAGE/#performance-optimization","title":"\ud83d\ude80 Performance Optimization","text":""},{"location":"USAGE/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Optimize for low-memory environments\nextractor = ChemistryReactionExtractor.from_pretrained(\n    \"chemplusx/rxnextract-complete\",\n    load_in_4bit=True,\n    device=\"cuda\",\n    max_memory={0: \"6GB\"},  # Limit GPU 0 to 6GB\n    offload_folder=\"./offload\"\n)\n\n# Clear cache periodically\nimport torch\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"USAGE/#batch-processing-optimization","title":"Batch Processing Optimization","text":"<pre><code>def optimized_batch_processing(procedures, batch_size=8):\n    \"\"\"Optimized batch processing with memory management\"\"\"\n\n    extractor = ChemistryReactionExtractor.from_pretrained(\n        \"chemplusx/rxnextract-complete\",\n        load_in_4bit=True\n    )\n\n    results = []\n    for i in range(0, len(procedures), batch_size):\n        batch = procedures[i:i+batch_size]\n\n        # Process batch\n        batch_results = extractor.batch_analyze(batch)\n        results.extend(batch_results)\n\n        # Clear memory\n        torch.cuda.empty_cache()\n\n        # Progress update\n        print(f\"Processed {min(i+batch_size, len(procedures))}/{len(procedures)} procedures\")\n\n    return results\n</code></pre> <p>Next Steps:  - For comprehensive analysis capabilities, see Analysis &amp; Evaluation Guide - For version history and updates, see Changelog - For contributing to the project, see Contributing Guidelines</p>"}]}